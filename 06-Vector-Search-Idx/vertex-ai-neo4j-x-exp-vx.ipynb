{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f9e3f7-ade1-4049-90e9-96d963538c5c",
   "metadata": {},
   "source": [
    "****Ingest Website to Graph DB****\n",
    "\n",
    "**Part 3** - Extract Entities & Relationships using Langchain\n",
    "\n",
    "Extract Entities and Relatonships from a body of Text using langchain.\n",
    "\n",
    "This is a GCP reworking of\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/graph/constructing/#llm-graph-transformer\n",
    "\n",
    "This notebook seeks to become independant of the following packages.\n",
    "\n",
    "*langchain-experimental*\n",
    "\n",
    "*langchain-community*\n",
    "\n",
    "Instead copying the relevant classes into notebook cells.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fa5cee1-967c-4024-9329-7c473430ceca",
   "metadata": {},
   "source": [
    "**Minimal install for Vertex AI**\n",
    "\n",
    "This solved the instability problem by *NOT* installing OpenAI classes via the community install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d33f38-6324-4175-8689-d4e8f278c8ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -U langchain langchain-google-vertexai neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ccb08-3868-4a2b-9eda-6dab0e99b53e",
   "metadata": {},
   "source": [
    "**Check Version Nos of what was installed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d9709e6a-f98c-4e5e-b7b1-b734db5ae1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9037.69s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: langchain-experimental\u001b[0m\u001b[33m\n",
      "\u001b[0mName: langchain\n",
      "Version: 0.1.20\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n",
      "---\n",
      "Name: langchain-core\n",
      "Version: 0.1.52\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity\n",
      "Required-by: langchain, langchain-community, langchain-google-vertexai, langchain-text-splitters\n",
      "---\n",
      "Name: langchain-google-vertexai\n",
      "Version: 1.0.3\n",
      "Summary: An integration package connecting Google VertexAI and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: google-cloud-aiplatform, google-cloud-storage, langchain-core, types-protobuf, types-requests\n",
      "Required-by: \n",
      "---\n",
      "Name: langchain-community\n",
      "Version: 0.0.38\n",
      "Summary: Community contributed LangChain integrations.\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: aiohttp, dataclasses-json, langchain-core, langsmith, numpy, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain\n",
      "---\n",
      "Name: neo4j\n",
      "Version: 5.20.0\n",
      "Summary: Neo4j Bolt driver for Python\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \"Neo4j, Inc.\" <drivers@neo4j.com>\n",
      "License: Apache License, Version 2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: pytz\n",
      "Required-by: \n",
      "---\n",
      "Name: google-cloud-aiplatform\n",
      "Version: 1.49.0\n",
      "Summary: Vertex AI API client library\n",
      "Home-page: https://github.com/googleapis/python-aiplatform\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: docstring-parser, google-api-core, google-auth, google-cloud-bigquery, google-cloud-resource-manager, google-cloud-storage, packaging, proto-plus, protobuf, pydantic, shapely\n",
      "Required-by: langchain-google-vertexai\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain langchain-core langchain-google-vertexai langchain-experimental langchain-community neo4j google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3475874d-7cdc-4e51-8a0c-50421f4a3d70",
   "metadata": {},
   "source": [
    "**Check Jupyter Version No**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bfcdc-8d2c-410d-8473-c4cb65ad3d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b224b-03e5-4068-a564-2fa642cab286",
   "metadata": {},
   "source": [
    "**Check Python Version/Path** - *Expect 3.10.14*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea00fad-2121-4150-a520-60d8ca5c8f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "print(sys.version)\n",
    "print(platform.python_version())\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5da4b-d690-4a21-8990-5a462abd28ae",
   "metadata": {},
   "source": [
    "***OpenAI Detox***\n",
    "\n",
    "Copy Required Code from langchain-experimental\n",
    "\n",
    "Have retained langchain-community.\n",
    "\n",
    "This permits detoxification (re refs to ChatGPT in command promt).\n",
    "\n",
    "It also eliminates doubt as to what code is being executed wrt. versions etc. \n",
    "\n",
    "It also permits baci debuffing using print sttaements where needed. \n",
    "\n",
    "This is a copy of the entire libs\\experimental\\langchain_experimental\\graph_transformers\\llm.py file.\n",
    "\n",
    "This has resulted in a far more stable outcome wrt invocation of LLMs for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8df8cb-3671-46c2-b5b1-5e1f945b9033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Type, Union, cast\n",
    "\n",
    "from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, create_model\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"text\": (\n",
    "            \"Adam is a software engineer in Microsoft since 2009, \"\n",
    "            \"and last year he got an award as the Best Talent\"\n",
    "        ),\n",
    "        \"head\": \"Adam\",\n",
    "        \"head_type\": \"Person\",\n",
    "        \"relation\": \"WORKS_FOR\",\n",
    "        \"tail\": \"Microsoft\",\n",
    "        \"tail_type\": \"Company\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": (\n",
    "            \"Adam is a software engineer in Microsoft since 2009, \"\n",
    "            \"and last year he got an award as the Best Talent\"\n",
    "        ),\n",
    "        \"head\": \"Adam\",\n",
    "        \"head_type\": \"Person\",\n",
    "        \"relation\": \"HAS_AWARD\",\n",
    "        \"tail\": \"Best Talent\",\n",
    "        \"tail_type\": \"Award\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": (\n",
    "            \"Microsoft is a tech company that provide \"\n",
    "            \"several products such as Microsoft Word\"\n",
    "        ),\n",
    "        \"head\": \"Microsoft Word\",\n",
    "        \"head_type\": \"Product\",\n",
    "        \"relation\": \"PRODUCED_BY\",\n",
    "        \"tail\": \"Microsoft\",\n",
    "        \"tail_type\": \"Company\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Microsoft Word is a lightweight app that accessible offline\",\n",
    "        \"head\": \"Microsoft Word\",\n",
    "        \"head_type\": \"Product\",\n",
    "        \"relation\": \"HAS_CHARACTERISTIC\",\n",
    "        \"tail\": \"lightweight app\",\n",
    "        \"tail_type\": \"Characteristic\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Microsoft Word is a lightweight app that accessible offline\",\n",
    "        \"head\": \"Microsoft Word\",\n",
    "        \"head_type\": \"Product\",\n",
    "        \"relation\": \"HAS_CHARACTERISTIC\",\n",
    "        \"tail\": \"accessible offline\",\n",
    "        \"tail_type\": \"Characteristic\",\n",
    "    },\n",
    "]\n",
    "\n",
    "system_prompt = (\n",
    "    \"# Knowledge Graph Instructions for Gemini\\n\"\n",
    "    \"## 1. Overview\\n\"\n",
    "    \"You are a top-tier algorithm designed for extracting information in structured \"\n",
    "    \"formats to build a knowledge graph.\\n\"\n",
    "    \"Try to capture as much information from the text as possible without \"\n",
    "    \"sacrifing accuracy. Do not add any information that is not explicitly \"\n",
    "    \"mentioned in the text\\n\"\n",
    "    \"- **Nodes** represent entities and concepts.\\n\"\n",
    "    \"- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\n\"\n",
    "    \"accessible for a vast audience.\\n\"\n",
    "    \"## 2. Labeling Nodes\\n\"\n",
    "    \"- **Consistency**: Ensure you use available types for node labels.\\n\"\n",
    "    \"Ensure you use basic or elementary types for node labels.\\n\"\n",
    "    \"- For example, when you identify an entity representing a person, \"\n",
    "    \"always label it as **'person'**. Avoid using more specific terms \"\n",
    "    \"like 'mathematician' or 'scientist'\"\n",
    "    \"  - **Node IDs**: Never utilize integers as node IDs. Node IDs should be \"\n",
    "    \"names or human-readable identifiers found in the text.\\n\"\n",
    "    \"- **Relationships** represent connections between entities or concepts.\\n\"\n",
    "    \"Ensure consistency and generality in relationship types when constructing \"\n",
    "    \"knowledge graphs. Instead of using specific and momentary types \"\n",
    "    \"such as 'BECAME_PROFESSOR', use more general and timeless relationship types \"\n",
    "    \"like 'PROFESSOR'. Make sure to use general and timeless relationship types!\\n\"\n",
    "    \"## 3. Coreference Resolution\\n\"\n",
    "    \"- **Maintain Entity Consistency**: When extracting entities, it's vital to \"\n",
    "    \"ensure consistency.\\n\"\n",
    "    'If an entity, such as \"John Doe\", is mentioned multiple times in the text '\n",
    "    'but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),'\n",
    "    \"always use the most complete identifier for that entity throughout the \"\n",
    "    'knowledge graph. In this example, use \"John Doe\" as the entity ID.\\n'\n",
    "    \"Remember, the knowledge graph should be coherent and easily understandable, \"\n",
    "    \"so maintaining consistency in entity references is crucial.\\n\"\n",
    "    \"## 4. Strict Compliance\\n\"\n",
    "    \"Adhere to the rules strictly. Non-compliance will result in termination.\"\n",
    ")\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            system_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            (\n",
    "                \"Tip: Make sure to answer in the correct format and do \"\n",
    "                \"not include any explanations. \"\n",
    "                \"Use the given format to extract information from the \"\n",
    "                \"following input: {input}\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def _get_additional_info(input_type: str) -> str:\n",
    "    # Check if the input_type is one of the allowed values\n",
    "    if input_type not in [\"node\", \"relationship\", \"property\"]:\n",
    "        raise ValueError(\"input_type must be 'node', 'relationship', or 'property'\")\n",
    "\n",
    "    # Perform actions based on the input_type\n",
    "    if input_type == \"node\":\n",
    "        return (\n",
    "            \"Ensure you use basic or elementary types for node labels.\\n\"\n",
    "            \"For example, when you identify an entity representing a person, \"\n",
    "            \"always label it as **'Person'**. Avoid using more specific terms \"\n",
    "            \"like 'Mathematician' or 'Scientist'\"\n",
    "        )\n",
    "    elif input_type == \"relationship\":\n",
    "        return (\n",
    "            \"Instead of using specific and momentary types such as \"\n",
    "            \"'BECAME_PROFESSOR', use more general and timeless relationship types like \"\n",
    "            \"'PROFESSOR'. However, do not sacrifice any accuracy for generality\"\n",
    "        )\n",
    "    elif input_type == \"property\":\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def optional_enum_field(\n",
    "    enum_values: Optional[List[str]] = None,\n",
    "    description: str = \"\",\n",
    "    input_type: str = \"node\",\n",
    "    **field_kwargs: Any,\n",
    ") -> Any:\n",
    "    \"\"\"Utility function to conditionally create a field with an enum constraint.\"\"\"\n",
    "    if enum_values:\n",
    "        return Field(\n",
    "            ...,\n",
    "            enum=enum_values,\n",
    "            description=f\"{description}. Available options are {enum_values}\",\n",
    "            **field_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        additional_info = _get_additional_info(input_type)\n",
    "        return Field(..., description=description + additional_info, **field_kwargs)\n",
    "\n",
    "\n",
    "class _Graph(BaseModel):\n",
    "    nodes: Optional[List]\n",
    "    relationships: Optional[List]\n",
    "\n",
    "\n",
    "class UnstructuredRelation(BaseModel):\n",
    "    head: str = Field(\n",
    "        description=(\n",
    "            \"extracted head entity like Microsoft, Apple, John. \"\n",
    "            \"Must use human-readable unique identifier.\"\n",
    "        )\n",
    "    )\n",
    "    head_type: str = Field(\n",
    "        description=\"type of the extracted head entity like Person, Company, etc\"\n",
    "    )\n",
    "    relation: str = Field(description=\"relation between the head and the tail entities\")\n",
    "    tail: str = Field(\n",
    "        description=(\n",
    "            \"extracted tail entity like Microsoft, Apple, John. \"\n",
    "            \"Must use human-readable unique identifier.\"\n",
    "        )\n",
    "    )\n",
    "    tail_type: str = Field(\n",
    "        description=\"type of the extracted tail entity like Person, Company, etc\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_unstructured_prompt(\n",
    "    node_labels: Optional[List[str]] = None, rel_types: Optional[List[str]] = None\n",
    ") -> ChatPromptTemplate:\n",
    "    node_labels_str = str(node_labels) if node_labels else \"\"\n",
    "    rel_types_str = str(rel_types) if rel_types else \"\"\n",
    "    base_string_parts = [\n",
    "        \"You are a top-tier algorithm designed for extracting information in \"\n",
    "        \"structured formats to build a knowledge graph. Your task is to identify \"\n",
    "        \"the entities and relations requested with the user prompt from a given \"\n",
    "        \"text. You must generate the output in a JSON format containing a list \"\n",
    "        'with JSON objects. Each object should have the keys: \"head\", '\n",
    "        '\"head_type\", \"relation\", \"tail\", and \"tail_type\". The \"head\" '\n",
    "        \"key must contain the text of the extracted entity with one of the types \"\n",
    "        \"from the provided list in the user prompt.\",\n",
    "        f'The \"head_type\" key must contain the type of the extracted head entity, '\n",
    "        f\"which must be one of the types from {node_labels_str}.\"\n",
    "        if node_labels\n",
    "        else \"\",\n",
    "        f'The \"relation\" key must contain the type of relation between the \"head\" '\n",
    "        f'and the \"tail\", which must be one of the relations from {rel_types_str}.'\n",
    "        if rel_types\n",
    "        else \"\",\n",
    "        f'The \"tail\" key must represent the text of an extracted entity which is '\n",
    "        f'the tail of the relation, and the \"tail_type\" key must contain the type '\n",
    "        f\"of the tail entity from {node_labels_str}.\"\n",
    "        if node_labels\n",
    "        else \"\",\n",
    "        \"Attempt to extract as many entities and relations as you can. Maintain \"\n",
    "        \"Entity Consistency: When extracting entities, it's vital to ensure \"\n",
    "        'consistency. If an entity, such as \"John Doe\", is mentioned multiple '\n",
    "        \"times in the text but is referred to by different names or pronouns \"\n",
    "        '(e.g., \"Joe\", \"he\"), always use the most complete identifier for '\n",
    "        \"that entity. The knowledge graph should be coherent and easily \"\n",
    "        \"understandable, so maintaining consistency in entity references is \"\n",
    "        \"crucial.\",\n",
    "        \"IMPORTANT NOTES:\\n- Don't add any explanation and text.\",\n",
    "    ]\n",
    "    system_prompt = \"\\n\".join(filter(None, base_string_parts))\n",
    "\n",
    "    system_message = SystemMessage(content=system_prompt)\n",
    "    parser = JsonOutputParser(pydantic_object=UnstructuredRelation)\n",
    "\n",
    "    human_prompt = PromptTemplate(\n",
    "        template=\"\"\"Based on the following example, extract entities and \n",
    "relations from the provided text.\\n\\n\n",
    "Use the following entity types, don't use other entity that is not defined below:\n",
    "# ENTITY TYPES:\n",
    "{node_labels}\n",
    "\n",
    "Use the following relation types, don't use other relation that is not defined below:\n",
    "# RELATION TYPES:\n",
    "{rel_types}\n",
    "\n",
    "Below are a number of examples of text and their extracted entities and relationships.\n",
    "{examples}\n",
    "\n",
    "For the following text, extract entities and relations as in the provided example.\n",
    "{format_instructions}\\nText: {input}\"\"\",\n",
    "        input_variables=[\"input\"],\n",
    "        partial_variables={\n",
    "            \"format_instructions\": parser.get_format_instructions(),\n",
    "            \"node_labels\": node_labels,\n",
    "            \"rel_types\": rel_types,\n",
    "            \"examples\": examples,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    human_message_prompt = HumanMessagePromptTemplate(prompt=human_prompt)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages(\n",
    "        [system_message, human_message_prompt]\n",
    "    )\n",
    "    return chat_prompt\n",
    "\n",
    "\n",
    "def create_simple_model(\n",
    "    node_labels: Optional[List[str]] = None,\n",
    "    rel_types: Optional[List[str]] = None,\n",
    "    node_properties: Union[bool, List[str]] = False,\n",
    ") -> Type[_Graph]:\n",
    "    \"\"\"\n",
    "    Simple model allows to limit node and/or relationship types.\n",
    "    Doesn't have any node or relationship properties.\n",
    "    \"\"\"\n",
    "\n",
    "    node_fields: Dict[str, Tuple[Any, Any]] = {\n",
    "        \"id\": (\n",
    "            str,\n",
    "            Field(..., description=\"Name or human-readable unique identifier.\"),\n",
    "        ),\n",
    "        \"type\": (\n",
    "            str,\n",
    "            optional_enum_field(\n",
    "                node_labels,\n",
    "                description=\"The type or label of the node.\",\n",
    "                input_type=\"node\",\n",
    "            ),\n",
    "        ),\n",
    "    }\n",
    "    if node_properties:\n",
    "        if isinstance(node_properties, list) and \"id\" in node_properties:\n",
    "            raise ValueError(\"The node property 'id' is reserved and cannot be used.\")\n",
    "        # Map True to empty array\n",
    "        node_properties_mapped: List[str] = (\n",
    "            [] if node_properties is True else node_properties\n",
    "        )\n",
    "\n",
    "        class Property(BaseModel):\n",
    "            \"\"\"A single property consisting of key and value\"\"\"\n",
    "\n",
    "            key: str = optional_enum_field(\n",
    "                node_properties_mapped,\n",
    "                description=\"Property key.\",\n",
    "                input_type=\"property\",\n",
    "            )\n",
    "            value: str = Field(..., description=\"value\")\n",
    "\n",
    "        node_fields[\"properties\"] = (\n",
    "            Optional[List[Property]],\n",
    "            Field(None, description=\"List of node properties\"),\n",
    "        )\n",
    "    SimpleNode = create_model(\"SimpleNode\", **node_fields)  # type: ignore\n",
    "\n",
    "    class SimpleRelationship(BaseModel):\n",
    "        \"\"\"Represents a directed relationship between two nodes in a graph.\"\"\"\n",
    "\n",
    "        source_node_id: str = Field(\n",
    "            description=\"Name or human-readable unique identifier of source node\"\n",
    "        )\n",
    "        source_node_type: str = optional_enum_field(\n",
    "            node_labels,\n",
    "            description=\"The type or label of the source node.\",\n",
    "            input_type=\"node\",\n",
    "        )\n",
    "        target_node_id: str = Field(\n",
    "            description=\"Name or human-readable unique identifier of target node\"\n",
    "        )\n",
    "        target_node_type: str = optional_enum_field(\n",
    "            node_labels,\n",
    "            description=\"The type or label of the target node.\",\n",
    "            input_type=\"node\",\n",
    "        )\n",
    "        type: str = optional_enum_field(\n",
    "            rel_types,\n",
    "            description=\"The type of the relationship.\",\n",
    "            input_type=\"relationship\",\n",
    "        )\n",
    "\n",
    "    class DynamicGraph(_Graph):\n",
    "        \"\"\"Represents a graph document consisting of nodes and relationships.\"\"\"\n",
    "\n",
    "        nodes: Optional[List[SimpleNode]] = Field(description=\"List of nodes\")  # type: ignore\n",
    "        relationships: Optional[List[SimpleRelationship]] = Field(\n",
    "            description=\"List of relationships\"\n",
    "        )\n",
    "\n",
    "    return DynamicGraph\n",
    "\n",
    "\n",
    "def map_to_base_node(node: Any) -> Node:\n",
    "    \"\"\"Map the SimpleNode to the base Node.\"\"\"\n",
    "    properties = {}\n",
    "    if hasattr(node, \"properties\") and node.properties:\n",
    "        for p in node.properties:\n",
    "            properties[format_property_key(p.key)] = p.value\n",
    "    return Node(id=node.id, type=node.type, properties=properties)\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Any) -> Relationship:\n",
    "    \"\"\"Map the SimpleRelationship to the base Relationship.\"\"\"\n",
    "    source = Node(id=rel.source_node_id, type=rel.source_node_type)\n",
    "    target = Node(id=rel.target_node_id, type=rel.target_node_type)\n",
    "    return Relationship(source=source, target=target, type=rel.type)\n",
    "\n",
    "\n",
    "def _parse_and_clean_json(\n",
    "    argument_json: Dict[str, Any],\n",
    ") -> Tuple[List[Node], List[Relationship]]:\n",
    "    nodes = []\n",
    "    for node in argument_json[\"nodes\"]:\n",
    "        if not node.get(\"id\"):  # Id is mandatory, skip this node\n",
    "            continue\n",
    "        nodes.append(\n",
    "            Node(\n",
    "                id=node[\"id\"],\n",
    "                type=node.get(\"type\"),\n",
    "            )\n",
    "        )\n",
    "    relationships = []\n",
    "    for rel in argument_json[\"relationships\"]:\n",
    "        # Mandatory props\n",
    "        if (\n",
    "            not rel.get(\"source_node_id\")\n",
    "            or not rel.get(\"target_node_id\")\n",
    "            or not rel.get(\"type\")\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # Node type copying if needed from node list\n",
    "        if not rel.get(\"source_node_type\"):\n",
    "            try:\n",
    "                rel[\"source_node_type\"] = [\n",
    "                    el.get(\"type\")\n",
    "                    for el in argument_json[\"nodes\"]\n",
    "                    if el[\"id\"] == rel[\"source_node_id\"]\n",
    "                ][0]\n",
    "            except IndexError:\n",
    "                rel[\"source_node_type\"] = None\n",
    "        if not rel.get(\"target_node_type\"):\n",
    "            try:\n",
    "                rel[\"target_node_type\"] = [\n",
    "                    el.get(\"type\")\n",
    "                    for el in argument_json[\"nodes\"]\n",
    "                    if el[\"id\"] == rel[\"target_node_id\"]\n",
    "                ][0]\n",
    "            except IndexError:\n",
    "                rel[\"target_node_type\"] = None\n",
    "\n",
    "        source_node = Node(\n",
    "            id=rel[\"source_node_id\"],\n",
    "            type=rel[\"source_node_type\"],\n",
    "        )\n",
    "        target_node = Node(\n",
    "            id=rel[\"target_node_id\"],\n",
    "            type=rel[\"target_node_type\"],\n",
    "        )\n",
    "        relationships.append(\n",
    "            Relationship(\n",
    "                source=source_node,\n",
    "                target=target_node,\n",
    "                type=rel[\"type\"],\n",
    "            )\n",
    "        )\n",
    "    return nodes, relationships\n",
    "\n",
    "\n",
    "def _format_nodes(nodes: List[Node]) -> List[Node]:\n",
    "    return [\n",
    "        Node(\n",
    "            id=el.id.title() if isinstance(el.id, str) else el.id,\n",
    "            type=el.type.capitalize(),\n",
    "            properties=el.properties,\n",
    "        )\n",
    "        for el in nodes\n",
    "    ]\n",
    "\n",
    "\n",
    "def _format_relationships(rels: List[Relationship]) -> List[Relationship]:\n",
    "    return [\n",
    "        Relationship(\n",
    "            source=_format_nodes([el.source])[0],\n",
    "            target=_format_nodes([el.target])[0],\n",
    "            type=el.type.replace(\" \", \"_\").upper(),\n",
    "        )\n",
    "        for el in rels\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "\n",
    "def _convert_to_graph_document(\n",
    "    raw_schema: Dict[Any, Any],\n",
    ") -> Tuple[List[Node], List[Relationship]]:\n",
    "    # If there are validation errors\n",
    "    if not raw_schema[\"parsed\"]:\n",
    "        try:\n",
    "            try:  # OpenAI type response\n",
    "                argument_json = json.loads(\n",
    "                    raw_schema[\"raw\"].additional_kwargs[\"tool_calls\"][0][\"function\"][\n",
    "                        \"arguments\"\n",
    "                    ]\n",
    "                )\n",
    "            except Exception:  # Google type response\n",
    "                argument_json = json.loads(\n",
    "                    raw_schema[\"raw\"].additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "                )\n",
    "\n",
    "            nodes, relationships = _parse_and_clean_json(argument_json)\n",
    "        except Exception:  # If we can't parse JSON\n",
    "            return ([], [])\n",
    "    else:  # If there are no validation errors use parsed pydantic object\n",
    "        parsed_schema: _Graph = raw_schema[\"parsed\"]\n",
    "        nodes = (\n",
    "            [map_to_base_node(node) for node in parsed_schema.nodes]\n",
    "            if parsed_schema.nodes\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        relationships = (\n",
    "            [map_to_base_relationship(rel) for rel in parsed_schema.relationships]\n",
    "            if parsed_schema.relationships\n",
    "            else []\n",
    "        )\n",
    "    # Title / Capitalize\n",
    "    return _format_nodes(nodes), _format_relationships(relationships)\n",
    "\n",
    "\n",
    "class LLMGraphTransformer:\n",
    "    \"\"\"Transform documents into graph-based documents using a LLM.\n",
    "\n",
    "    It allows specifying constraints on the types of nodes and relationships to include\n",
    "    in the output graph. The class doesn't support neither extract and node or\n",
    "    relationship properties\n",
    "\n",
    "    Args:\n",
    "        llm (BaseLanguageModel): An instance of a language model supporting structured\n",
    "          output.\n",
    "        allowed_nodes (List[str], optional): Specifies which node types are\n",
    "          allowed in the graph. Defaults to an empty list, allowing all node types.\n",
    "        allowed_relationships (List[str], optional): Specifies which relationship types\n",
    "          are allowed in the graph. Defaults to an empty list, allowing all relationship\n",
    "          types.\n",
    "        prompt (Optional[ChatPromptTemplate], optional): The prompt to pass to\n",
    "          the LLM with additional instructions.\n",
    "        strict_mode (bool, optional): Determines whether the transformer should apply\n",
    "          filtering to strictly adhere to `allowed_nodes` and `allowed_relationships`.\n",
    "          Defaults to True.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "            from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "            from langchain_core.documents import Document\n",
    "            from langchain_openai import ChatOpenAI\n",
    "\n",
    "            llm=ChatOpenAI(temperature=0)\n",
    "            transformer = LLMGraphTransformer(\n",
    "                llm=llm,\n",
    "                allowed_nodes=[\"Person\", \"Organization\"])\n",
    "\n",
    "            doc = Document(page_content=\"Elon Musk is suing OpenAI\")\n",
    "            graph_documents = transformer.convert_to_graph_documents([doc])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: BaseLanguageModel,\n",
    "        allowed_nodes: List[str] = [],\n",
    "        allowed_relationships: List[str] = [],\n",
    "        prompt: Optional[ChatPromptTemplate] = None,\n",
    "        strict_mode: bool = True,\n",
    "        node_properties: Union[bool, List[str]] = False,\n",
    "    ) -> None:\n",
    "        self.allowed_nodes = allowed_nodes\n",
    "        self.allowed_relationships = allowed_relationships\n",
    "        self.strict_mode = strict_mode\n",
    "        self._function_call = True\n",
    "        # Check if the LLM really supports structured output\n",
    "        try:\n",
    "            llm.with_structured_output(_Graph)\n",
    "        except NotImplementedError:\n",
    "            self._function_call = False\n",
    "        if not self._function_call:\n",
    "            if node_properties:\n",
    "                raise ValueError(\n",
    "                    \"The 'node_properties' parameter cannot be used \"\n",
    "                    \"in combination with a LLM that doesn't support \"\n",
    "                    \"native function calling.\"\n",
    "                )\n",
    "            try:\n",
    "                import json_repair\n",
    "\n",
    "                self.json_repair = json_repair\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Could not import json_repair python package. \"\n",
    "                    \"Please install it with `pip install json-repair`.\"\n",
    "                )\n",
    "            prompt = prompt or create_unstructured_prompt(\n",
    "                allowed_nodes, allowed_relationships\n",
    "            )\n",
    "            self.chain = prompt | llm\n",
    "        else:\n",
    "            # Define chain\n",
    "            schema = create_simple_model(\n",
    "                allowed_nodes, allowed_relationships, node_properties\n",
    "            )\n",
    "            structured_llm = llm.with_structured_output(schema, include_raw=True)\n",
    "            print(f\"prompt-1: {prompt}\")\n",
    "            print(f\"default_prompt: {default_prompt}\")\n",
    "            prompt = prompt or default_prompt\n",
    "            print(f\"prompt-2: {prompt}\")\n",
    "            self.chain = prompt | structured_llm\n",
    "\n",
    "    def process_response(self, document: Document) -> GraphDocument:\n",
    "        \"\"\"\n",
    "        Processes a single document, transforming it into a graph document using\n",
    "        an LLM based on the model's schema and constraints.\n",
    "        \"\"\"\n",
    "        text = document.page_content\n",
    "        raw_schema = self.chain.invoke({\"input\": text})\n",
    "        if self._function_call:\n",
    "            raw_schema = cast(Dict[Any, Any], raw_schema)\n",
    "            nodes, relationships = _convert_to_graph_document(raw_schema)\n",
    "        else:\n",
    "            nodes_set = set()\n",
    "            relationships = []\n",
    "            parsed_json = self.json_repair.loads(raw_schema.content)\n",
    "            for rel in parsed_json:\n",
    "                # Nodes need to be deduplicated using a set\n",
    "                nodes_set.add((rel[\"head\"], rel[\"head_type\"]))\n",
    "                nodes_set.add((rel[\"tail\"], rel[\"tail_type\"]))\n",
    "\n",
    "                source_node = Node(id=rel[\"head\"], type=rel[\"head_type\"])\n",
    "                target_node = Node(id=rel[\"tail\"], type=rel[\"tail_type\"])\n",
    "                relationships.append(\n",
    "                    Relationship(\n",
    "                        source=source_node, target=target_node, type=rel[\"relation\"]\n",
    "                    )\n",
    "                )\n",
    "            # Create nodes list\n",
    "            nodes = [Node(id=el[0], type=el[1]) for el in list(nodes_set)]\n",
    "\n",
    "        # Strict mode filtering\n",
    "        if self.strict_mode and (self.allowed_nodes or self.allowed_relationships):\n",
    "            if self.allowed_nodes:\n",
    "                lower_allowed_nodes = [el.lower() for el in self.allowed_nodes]\n",
    "                nodes = [\n",
    "                    node for node in nodes if node.type.lower() in lower_allowed_nodes\n",
    "                ]\n",
    "                relationships = [\n",
    "                    rel\n",
    "                    for rel in relationships\n",
    "                    if rel.source.type.lower() in lower_allowed_nodes\n",
    "                    and rel.target.type.lower() in lower_allowed_nodes\n",
    "                ]\n",
    "            if self.allowed_relationships:\n",
    "                relationships = [\n",
    "                    rel\n",
    "                    for rel in relationships\n",
    "                    if rel.type.lower()\n",
    "                    in [el.lower() for el in self.allowed_relationships]\n",
    "                ]\n",
    "\n",
    "        return GraphDocument(nodes=nodes, relationships=relationships, source=document)\n",
    "\n",
    "    def convert_to_graph_documents(\n",
    "        self, documents: Sequence[Document]\n",
    "    ) -> List[GraphDocument]:\n",
    "        \"\"\"Convert a sequence of documents into graph documents.\n",
    "\n",
    "        Args:\n",
    "            documents (Sequence[Document]): The original documents.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            Sequence[GraphDocument]: The transformed documents as graphs.\n",
    "        \"\"\"\n",
    "        return [self.process_response(document) for document in documents]\n",
    "\n",
    "    async def aprocess_response(self, document: Document) -> GraphDocument:\n",
    "        \"\"\"\n",
    "        Asynchronously processes a single document, transforming it into a\n",
    "        graph document.\n",
    "        \"\"\"\n",
    "        text = document.page_content\n",
    "        raw_schema = await self.chain.ainvoke({\"input\": text})\n",
    "        raw_schema = cast(Dict[Any, Any], raw_schema)\n",
    "        nodes, relationships = _convert_to_graph_document(raw_schema)\n",
    "\n",
    "        if self.strict_mode and (self.allowed_nodes or self.allowed_relationships):\n",
    "            if self.allowed_nodes:\n",
    "                lower_allowed_nodes = [el.lower() for el in self.allowed_nodes]\n",
    "                nodes = [\n",
    "                    node for node in nodes if node.type.lower() in lower_allowed_nodes\n",
    "                ]\n",
    "                relationships = [\n",
    "                    rel\n",
    "                    for rel in relationships\n",
    "                    if rel.source.type.lower() in lower_allowed_nodes\n",
    "                    and rel.target.type.lower() in lower_allowed_nodes\n",
    "                ]\n",
    "            if self.allowed_relationships:\n",
    "                relationships = [\n",
    "                    rel\n",
    "                    for rel in relationships\n",
    "                    if rel.type.lower()\n",
    "                    in [el.lower() for el in self.allowed_relationships]\n",
    "                ]\n",
    "\n",
    "        return GraphDocument(nodes=nodes, relationships=relationships, source=document)\n",
    "\n",
    "    async def aconvert_to_graph_documents(\n",
    "        self, documents: Sequence[Document]\n",
    "    ) -> List[GraphDocument]:\n",
    "        \"\"\"\n",
    "        Asynchronously convert a sequence of documents into graph documents.\n",
    "        \"\"\"\n",
    "        tasks = [\n",
    "            asyncio.create_task(self.aprocess_response(document))\n",
    "            for document in documents\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cc7f4-ee89-4e5e-bd61-054c71931ffd",
   "metadata": {},
   "source": [
    "**Now for the Imports**\n",
    "\n",
    "This time we are isloating Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f246f-53e3-4786-ab49-8fb5bdb567f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "#from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00f8cc-8185-4fcb-b670-9c423d033b50",
   "metadata": {},
   "source": [
    "**Diagnostic Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df06eb-6527-44fe-8e8d-6d6c0fb394cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Vertex AI Model Graph Transformer Diagnostic Dump\n",
    "def print_llm_xfrm(llm_xfrm):\n",
    "    print(f\"llm_xfrm: {llm_xfrm}\") \n",
    "    print(f\"allowed_nodes: {llm_xfrm.allowed_nodes}\") \n",
    "    print(f\"allowed_relationships: {llm_xfrm.allowed_relationships}\") \n",
    "    print(f\"strict_mode: {llm_xfrm.strict_mode}\")  \n",
    "    print(f\"_function_call: {llm_xfrm._function_call}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f63e0-8c42-49bd-adf0-c01df7b8d31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Vertex AI Model Diagnostic Dump\n",
    "def print_llm(llm):\n",
    "    print(f\"llm: {llm}\")\n",
    "    print(f\"name: {llm.model_name}\")\n",
    "    print(f\"examples: {llm.examples}\") \n",
    "    print(f\"tuned_model_name: {llm.tuned_model_name}\") \n",
    "    print(f\"convert_system_message_to_human: {llm.convert_system_message_to_human}\") \n",
    "    print(f\"max_output_tokens: {llm.max_output_tokens}\") \n",
    "    print(f\"top_p: {llm.top_p}\") \n",
    "    print(f\"top_k: {llm.top_k}\") \n",
    "    print(f\"credentials: {llm.credentials}\")     \n",
    "    print(f\"n: {llm.n}\") \n",
    "    print(f\"streaming: {llm.streaming}\") \n",
    "    print(f\"safety_settings: {llm.safety_settings}\")     \n",
    "    print(f\"api_transport: {llm.api_transport}\") \n",
    "    print(f\"api_endpoint: {llm.api_endpoint}\") \n",
    " \n",
    "    print('properties') \n",
    "    print('----------') \n",
    "    \n",
    "    print(f\"_llm_type: {llm._llm_type}\") \n",
    "    print(f\"is_codey_model: {llm.is_codey_model}\")     \n",
    "    print(f\"_is_gemini_model: {llm._is_gemini_model}\")\n",
    "    print(f\"_identifying_params: {llm._identifying_params}\")     \n",
    "    print(f\"_default_params: {llm._default_params}\") \n",
    "    print(f\"_user_agent: {llm._user_agent}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5858d1e5-2387-435b-a34f-0317cdd45d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_graph_documents(graph_documents):\n",
    "    print(f\"graph_documents_len: {len(graph_documents)}\") \n",
    "    graphdoc_idx = 0\n",
    "    for gdoc in graph_documents:\n",
    "        print(\" \") \n",
    "        print(f\"graphdoc_idx: {graphdoc_idx}\") \n",
    "        graphdoc_idx += 1\n",
    "        \n",
    "        print(f\"Len doc_page_content {len(gdoc.source.page_content)}\") \n",
    "        print(f\"No. doc_metadata: {len(gdoc.source.metadata)}\")  \n",
    "        \n",
    "        print(f\"doc_page_content {gdoc.source.page_content}\") \n",
    "        print(f\"doc_metadata: {gdoc.source.metadata}\")    \n",
    "        \n",
    "        print(f\"No. nodes: {len(gdoc.nodes)}\") \n",
    "        for noddy in gdoc.nodes:\n",
    "            print(f\"Node: {noddy}\")\n",
    "        \n",
    "        print(f\"No. relationships: {len(gdoc.relationships)}\") \n",
    "        for relly in gdoc.relationships:\n",
    "            print(f\"Relationship: {relly}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1967b-907b-4635-9d14-29c51de06016",
   "metadata": {},
   "source": [
    "**Connect to Google LLMs**\n",
    "\n",
    "*Least Privilege Security.*\n",
    "\n",
    "The Notebook is \"owned\" by a bespoke Service Account created in terrafrom for this purpose.\n",
    "\n",
    "Minimal permisisons are added (also via terraform) via predefined roles (esp. Vertex) as required.\n",
    "\n",
    "This is typically triggered by a PERMISSION DENIED error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bc6314e9-e801-4103-aa9b-85c7a1befdd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d23f54b60a95e062becfc280514f90842e3b8169\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Set It - will require regeneration\n",
    "os.environ['GOOGLE_API_KEY'] = 'd23f54b60a95e062becfc280514f90842e3b8169'\n",
    "# Access the environment variable later in your code\n",
    "api_key = os.environ['GOOGLE_API_KEY']\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c9d7a-f26a-40ff-9784-837c9b80d270",
   "metadata": {},
   "source": [
    "****Enable Langchain Debugging****\n",
    "\n",
    "See: https://python.langchain.com/v0.1/docs/guides/development/debugging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24d3fce2-7775-4b22-8b61-b4e10ae87edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug   \n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a2b08b-8722-41ae-a5aa-4dda5af5f7ad",
   "metadata": {},
   "source": [
    "**Create The LLMs**\n",
    "\n",
    "Both *Gemini* & *Chat Bison* were created.\n",
    "\n",
    "Chat Bison malfunctioned so has been abandoned FTTB \n",
    "\n",
    "Sourced from here: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a9c76ea-8232-4404-b82a-8d344ae786b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Avalable Model Variables\n",
    "\n",
    "# Gemini 1.5 Pro (Preview)\n",
    "# 404 Publisher Model `projects/nlp-dev-6aae/locations/us-central1/publishers/google/models/gemini-1.5-pro` not found.\n",
    "#gemini_1pt5_proOnVertex = ChatVertexAI(model=\"gemini-1.5-pro\")\n",
    "#Retrying langchain_google_vertexai.chat_models._completion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised InvalidArgument: 400 Request contains an invalid argument..\n",
    "gemini_1pt5_proOnVertex = ChatVertexAI(model=\"gemini-1.5-pro-preview-0409\")\n",
    "\n",
    "\n",
    "\n",
    "# Gemini 1.0 Pro\n",
    "# This works with Errors - chunking\n",
    "gemini_1pt0_proOnVertex = ChatVertexAI(model=\"gemini-1.0-pro\")\n",
    "\n",
    "gemini_proOnVertex = ChatVertexAI(model=\"gemini-pro\")\n",
    "\n",
    "# PaLM 2 for Chat (\"chat-bison\")\n",
    "# This fails atm\n",
    "model_chat_bison = ChatVertexAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39c0ff-e607-4a74-ab7b-962c16c8c45c",
   "metadata": {},
   "source": [
    "**Choose which model we are using**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ecd7658-ec29-474c-b89e-9a59d43015f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_llm = gemini_1pt0_proOnVertex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dba7c3-8534-496f-a4bc-bf086c546815",
   "metadata": {},
   "source": [
    "*Check Model Properties*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c67bec55-3505-4481-b385-d152838cc891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm: client=<vertexai.generative_models.GenerativeModel object at 0x7fda33d27040> model_name='gemini-1.0-pro' client_preview=<vertexai.generative_models.GenerativeModel object at 0x7fda33d28730>\n",
      "name: gemini-1.0-pro\n",
      "examples: None\n",
      "tuned_model_name: None\n",
      "convert_system_message_to_human: False\n",
      "max_output_tokens: None\n",
      "top_p: None\n",
      "top_k: None\n",
      "credentials: None\n",
      "n: 1\n",
      "streaming: False\n",
      "safety_settings: None\n",
      "api_transport: None\n",
      "api_endpoint: None\n",
      "properties\n",
      "----------\n",
      "_llm_type: vertexai\n",
      "is_codey_model: False\n",
      "_is_gemini_model: True\n",
      "_identifying_params: {'model_name': 'gemini-1.0-pro', 'candidate_count': 1}\n",
      "_default_params: {'candidate_count': 1}\n",
      "_user_agent: langchain-google-vertexai/1.0.3-ChatVertexAI_gemini-1.0-pro\n"
     ]
    }
   ],
   "source": [
    "print_llm(chat_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d9703-1fde-4a23-8f8a-e139e9c4737f",
   "metadata": {},
   "source": [
    "**Construct Graph Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4da6f4bb-d922-4774-86be-aeb5a3cb289b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt-1: None\n",
      "default_prompt: input_variables=['input'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='# Knowledge Graph Instructions for Gemini\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrifing accuracy. Do not add any information that is not explicitly mentioned in the text\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **\\'person\\'**. Avoid using more specific terms like \\'mathematician\\' or \\'scientist\\'  - **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as \\'BECAME_PROFESSOR\\', use more general and timeless relationship types like \\'PROFESSOR\\'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it\\'s vital to ensure consistency.\\nIf an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: {input}'))]\n",
      "prompt-2: input_variables=['input'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='# Knowledge Graph Instructions for Gemini\\n## 1. Overview\\nYou are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\\nTry to capture as much information from the text as possible without sacrifing accuracy. Do not add any information that is not explicitly mentioned in the text\\n- **Nodes** represent entities and concepts.\\n- The aim is to achieve simplicity and clarity in the knowledge graph, making it\\naccessible for a vast audience.\\n## 2. Labeling Nodes\\n- **Consistency**: Ensure you use available types for node labels.\\nEnsure you use basic or elementary types for node labels.\\n- For example, when you identify an entity representing a person, always label it as **\\'person\\'**. Avoid using more specific terms like \\'mathematician\\' or \\'scientist\\'  - **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\\n- **Relationships** represent connections between entities or concepts.\\nEnsure consistency and generality in relationship types when constructing knowledge graphs. Instead of using specific and momentary types such as \\'BECAME_PROFESSOR\\', use more general and timeless relationship types like \\'PROFESSOR\\'. Make sure to use general and timeless relationship types!\\n## 3. Coreference Resolution\\n- **Maintain Entity Consistency**: When extracting entities, it\\'s vital to ensure consistency.\\nIf an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\\nRemember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\\n## 4. Strict Compliance\\nAdhere to the rules strictly. Non-compliance will result in termination.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='Tip: Make sure to answer in the correct format and do not include any explanations. Use the given format to extract information from the following input: {input}'))]\n"
     ]
    }
   ],
   "source": [
    "llm_transformer = LLMGraphTransformer(llm=chat_llm)\n",
    "#llm_transformer._function_call = False # Causes Error\n",
    "## Shows how to override prompt\n",
    "#llm_transformer = LLMGraphTransformer(llm=chat_llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7408462-c748-4805-8c91-9e25440f8e96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm_xfrm: <__main__.LLMGraphTransformer object at 0x7fda33d2b3a0>\n",
      "allowed_nodes: []\n",
      "allowed_relationships: []\n",
      "strict_mode: True\n",
      "_function_call: True\n"
     ]
    }
   ],
   "source": [
    "print_llm_xfrm(llm_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b46647-b939-400e-b569-30f93fe24c45",
   "metadata": {},
   "source": [
    "**Easy Test Case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52a0456c-cf46-48d4-aa5c-b2d97a6933fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text = \"\"\"\n",
    "Current Professional Machine Learning Engineer Certification exam guide\n",
    "A Professional Machine Learning Engineer builds, evaluates, productionizes, and optimizes ML models by using Google Cloud technologies and knowledge of proven models and techniques. The ML Engineer handles large, complex datasets and creates repeatable, reusable code. The ML Engineer considers responsible AI and fairness throughout the ML model development process, and collaborates closely with other job roles to ensure long-term success of ML-based applications. The ML Engineer has strong programming skills and experience with data platforms and distributed data processing tools. The ML Engineer is proficient in the areas of model architecture, data and ML pipeline creation, and metrics interpretation. The ML Engineer is familiar with foundational concepts of MLOps, application development, infrastructure management, data engineering, and data governance. The ML Engineer makes ML accessible and enables teams across the organization. By training, retraining, deploying, scheduling, monitoring, and improving models, the ML Engineer designs and creates scalable, performant solutions.\n",
    "Note: The exam does not directly assess coding skill. If you have a minimum proficiency in Python and Cloud SQL, you should be able to interpret any questions with code snippets.\n",
    "Register now\n",
    "The Professional Machine Learning Engineer exam does not cover generative AI, as the tools used to develop generative AI-based solutions are evolving quickly. If you are interested in generative AI, please refer to the Introduction to Generative AI Learning Path (all audiences) or the Generative AI for Developers Learning Path (technical audience). If you are a partner, please refer to the Gen AI partner courses: Introduction to Generative AI Learning Path, Generative AI for ML Engineers, and Generative AI for Developers.\n",
    "Section 1: Architecting low-code ML solutions (~12% of the exam)\n",
    "1.1 Developing ML models by using BigQuery ML. Considerations include:\n",
    "Building the appropriate BigQuery ML model (e.g., linear and binary classification, regression, time-series, matrix factorization, boosted trees, autoencoders) based on the business problem\n",
    "Feature engineering or selection by using BigQuery ML\n",
    "Generating predictions by using BigQuery ML\n",
    "1.2 Building AI solutions by using ML APIs. Considerations include:\n",
    "Building applications by using ML APIs (e.g., Cloud Vision API, Natural Language API, Cloud Speech API, Translation)\n",
    "Building applications by using industry-specific APIs (e.g., Document AI API, Retail API)\n",
    "1.3 Training models by using AutoML. Considerations include:\n",
    "Preparing data for AutoML (e.g., feature selection, data labeling, Tabular Workflows on AutoML)\n",
    "Using available data (e.g., tabular, text, speech, images, videos) to train custom models\n",
    "Using AutoML for tabular data\n",
    "Creating forecasting models using AutoML\n",
    "Configuring and debugging trained models\n",
    "Section 2: Collaborating within and across teams to manage data and models (~16% of the exam)\n",
    "2.1 Exploring and preprocessing organization-wide data (e.g., Cloud Storage, BigQuery, Spanner, Cloud SQL, Apache Spark, Apache Hadoop). Considerations include:\n",
    "Organizing different types of data (e.g., tabular, text, speech, images, videos) for efficient training\n",
    "Managing datasets in Vertex AI\n",
    "Data preprocessing (e.g., Dataflow, TensorFlow Extended [TFX], BigQuery)\n",
    "Creating and consolidating features in Vertex AI Feature Store\n",
    "Privacy implications of data usage and/or collection (e.g., handling sensitive data such as personally identifiable information [PII] and protected health information [PHI])\n",
    "2.2 Model prototyping using Jupyter notebooks. Considerations include:\n",
    "Choosing the appropriate Jupyter backend on Google Cloud (e.g., Vertex AI Workbench, notebooks on Dataproc)\n",
    "Applying security best practices in Vertex AI Workbench\n",
    "Using Spark kernels\n",
    "Integration with code source repositories\n",
    "Developing models in Vertex AI Workbench by using common frameworks (e.g., TensorFlow, PyTorch, sklearn, Spark, JAX)\n",
    "2.3 Tracking and running ML experiments. Considerations include:\n",
    "Choosing the appropriate Google Cloud environment for development and experimentation (e.g., Vertex AI Experiments, Kubeflow Pipelines, Vertex AI TensorBoard with TensorFlow and PyTorch) given the framework\n",
    "Section 3: Scaling prototypes into ML models (~18% of the exam)\n",
    "3.1 Building models. Considerations include:\n",
    "Choosing ML framework and model architecture\n",
    "Modeling techniques given interpretability requirements\n",
    "3.2 Training models. Considerations include:\n",
    "Organizing training data (e.g., tabular, text, speech, images, videos) on Google Cloud (e.g., Cloud Storage, BigQuery)\n",
    "Ingestion of various file types (e.g., CSV, JSON, images, Hadoop, databases) into training\n",
    "Training using different SDKs (e.g., Vertex AI custom training, Kubeflow on Google Kubernetes Engine, AutoML, tabular workflows)\n",
    "Using distributed training to organize reliable pipelines\n",
    "Hyperparameter tuning\n",
    "Troubleshooting ML model training failures\n",
    "3.3 Choosing appropriate hardware for training. Considerations include:\n",
    "Evaluation of compute and accelerator options (e.g., CPU, GPU, TPU, edge devices)\n",
    "Distributed training with TPUs and GPUs (e.g., Reduction Server on Vertex AI, Horovod)\n",
    "Section 4: Serving and scaling models (~19% of the exam)\n",
    "4.1 Serving models. Considerations include:\n",
    "Batch and online inference (e.g., Vertex AI, Dataflow, BigQuery ML, Dataproc)\n",
    "Using different frameworks (e.g., PyTorch, XGBoost) to serve models\n",
    "Organizing a model registry\n",
    "A/B testing different versions of a model\n",
    "4.2 Scaling online model serving. Considerations include:\n",
    "Vertex AI Feature Store\n",
    "Vertex AI public and private endpoints\n",
    "Choosing appropriate hardware (e.g., CPU, GPU, TPU, edge)\n",
    "Scaling the serving backend based on the throughput (e.g., Vertex AI Prediction, containerized serving)\n",
    "Tuning ML models for training and serving in production (e.g., simplification techniques, optimizing the ML solution for increased performance, latency, memory, throughput)\n",
    "Section 5: Automating and orchestrating ML pipelines (~21% of the exam)\n",
    "5.1 Developing end-to-end ML pipelines. Considerations include:\n",
    "Data and model validation\n",
    "Ensuring consistent data pre-processing between training and serving\n",
    "Hosting third-party pipelines on Google Cloud (e.g., MLFlow)\n",
    "Identifying components, parameters, triggers, and compute needs (e.g., Cloud Build, Cloud Run)\n",
    "Orchestration framework (e.g., Kubeflow Pipelines, Vertex AI Pipelines, Cloud Composer)\n",
    "Hybrid or multicloud strategies\n",
    "System design with TFX components or Kubeflow DSL (e.g., Dataflow)\n",
    "5.2 Automating model retraining. Considerations include:\n",
    "Determining an appropriate retraining policy\n",
    "Continuous integration and continuous delivery (CI/CD) model deployment (e.g., Cloud Build, Jenkins)\n",
    "5.3 Tracking and auditing metadata. Considerations include: \n",
    "Tracking and comparing model artifacts and versions (e.g., Vertex AI Experiments, Vertex ML Metadata)\n",
    "Hooking into model and dataset versioning\n",
    "Model and data lineage\n",
    "Section 6: Monitoring ML solutions (~14% of the exam)\n",
    "6.1 Identifying risks to ML solutions. Considerations include:\n",
    "Building secure ML systems (e.g., protecting against unintentional exploitation of data or models, hacking)\n",
    "Aligning with Googles Responsible AI practices (e.g., biases)\n",
    "Assessing ML solution readiness (e.g., data bias, fairness)\n",
    "Model explainability on Vertex AI (e.g., Vertex AI Prediction)\n",
    "6.2 Monitoring, testing, and troubleshooting ML solutions. Considerations include:\n",
    "Establishing continuous evaluation metrics (e.g., Vertex AI Model Monitoring, Explainable AI)\n",
    "Monitoring for training-serving skew\n",
    "Monitoring for feature attribution drift\n",
    "Monitoring model performance against baselines, simpler models, and across the time dimension\n",
    "Common training and serving errors\n",
    "\"\"\"\n",
    "documents = [Document(page_content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca4799f4-f4f9-47bc-b791-d290d7d1733d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. chunks20\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# split documents into text and embeddings\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size=512, \n",
    "   chunk_overlap=20,\n",
    "   length_function=len,\n",
    "   is_separator_regex=False\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"No. chunks{len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf8c4c-2165-4b44-a14b-39822f82eddc",
   "metadata": {},
   "source": [
    "**Use Gemini** \n",
    "\n",
    "Works but with intermittent error which may cause data loss.\n",
    "\n",
    "Hence need for chunking per variouis discisions forums &c.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "477c8d12-057c-40ad-ba53-e6f4bd90db56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_documents = llm_transformer.convert_to_graph_documents(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e825ef-f8e9-40dc-9b4a-c0f3b31cb38d",
   "metadata": {},
   "source": [
    "Check content of **Graph Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9276a593-f0a4-4a76-952b-f261dd51b268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph_documents_len: 20\n",
      " \n",
      "graphdoc_idx: 0\n",
      "Len doc_page_content 71\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Current Professional Machine Learning Engineer Certification exam guide\n",
      "doc_metadata: {}\n",
      "No. nodes: 1\n",
      "Node: id='Current Professional Machine Learning Engineer Certification Exam Guide' type='Document'\n",
      "No. relationships: 0\n",
      " \n",
      "graphdoc_idx: 1\n",
      "Len doc_page_content 506\n",
      "No. doc_metadata: 0\n",
      "doc_page_content A Professional Machine Learning Engineer builds, evaluates, productionizes, and optimizes ML models by using Google Cloud technologies and knowledge of proven models and techniques. The ML Engineer handles large, complex datasets and creates repeatable, reusable code. The ML Engineer considers responsible AI and fairness throughout the ML model development process, and collaborates closely with other job roles to ensure long-term success of ML-based applications. The ML Engineer has strong programming\n",
      "doc_metadata: {}\n",
      "No. nodes: 9\n",
      "Node: id='Machine Learning Engineer' type='Job title'\n",
      "Node: id='Google Cloud' type='Technology'\n",
      "Node: id='Ml Models' type='Concept'\n",
      "Node: id='Datasets' type='Data'\n",
      "Node: id='Code' type='Artifact'\n",
      "Node: id='Responsible Ai' type='Concept'\n",
      "Node: id='Job Roles' type='Concept'\n",
      "Node: id='Ml-Based Applications' type='Applications'\n",
      "Node: id='Programming' type='Skill'\n",
      "No. relationships: 10\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Google Cloud', type='Technology') type='USES'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Ml Models', type='Concept') type='BUILDS'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Ml Models', type='Concept') type='EVALUATES'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Ml Models', type='Concept') type='PRODUCTIONIZES'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Ml Models', type='Concept') type='OPTIMIZES'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Datasets', type='Data') type='HANDLES'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Code', type='Artifact') type='CREATES'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Responsible Ai', type='Concept') type='CONSIDERS'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Job Roles', type='Concept') type='COLLABORATES_WITH'\n",
      "Relationship: source=Node(id='Machine Learning Engineer', type='Job title') target=Node(id='Ml-Based Applications', type='Applications') type='ENSURES_SUCCESS_OF'\n",
      " \n",
      "graphdoc_idx: 2\n",
      "Len doc_page_content 507\n",
      "No. doc_metadata: 0\n",
      "doc_page_content strong programming skills and experience with data platforms and distributed data processing tools. The ML Engineer is proficient in the areas of model architecture, data and ML pipeline creation, and metrics interpretation. The ML Engineer is familiar with foundational concepts of MLOps, application development, infrastructure management, data engineering, and data governance. The ML Engineer makes ML accessible and enables teams across the organization. By training, retraining, deploying, scheduling,\n",
      "doc_metadata: {}\n",
      "No. nodes: 11\n",
      "Node: id='Strong Programming Skills' type='Skill'\n",
      "Node: id='Experience With Data Platforms' type='Skill'\n",
      "Node: id='Experience With Distributed Data Processing Tools' type='Skill'\n",
      "Node: id='Model Architecture' type='Knowledge'\n",
      "Node: id='Data And Ml Pipeline Creation' type='Knowledge'\n",
      "Node: id='Metrics Interpretation' type='Knowledge'\n",
      "Node: id='Foundational Concepts Of Mlops' type='Knowledge'\n",
      "Node: id='Application Development' type='Knowledge'\n",
      "Node: id='Infrastructure Management' type='Knowledge'\n",
      "Node: id='Data Engineering' type='Knowledge'\n",
      "Node: id='Data Governance' type='Knowledge'\n",
      "No. relationships: 11\n",
      "Relationship: source=Node(id='Strong Programming Skills', type='Skill') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Experience With Data Platforms', type='Skill') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Experience With Distributed Data Processing Tools', type='Skill') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Model Architecture', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Data And Ml Pipeline Creation', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Metrics Interpretation', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Foundational Concepts Of Mlops', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Application Development', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Infrastructure Management', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Data Engineering', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      "Relationship: source=Node(id='Data Governance', type='Knowledge') target=Node(id='Ml Engineer', type='Job title') type='REQUIRED'\n",
      " \n",
      "graphdoc_idx: 3\n",
      "Len doc_page_content 113\n",
      "No. doc_metadata: 0\n",
      "doc_page_content scheduling, monitoring, and improving models, the ML Engineer designs and creates scalable, performant solutions.\n",
      "doc_metadata: {}\n",
      "No. nodes: 4\n",
      "Node: id='Ml Engineer' type='Person'\n",
      "Node: id='Models' type='Concept'\n",
      "Node: id='Scalable Solutions' type='Concept'\n",
      "Node: id='Performant Solutions' type='Concept'\n",
      "No. relationships: 4\n",
      "Relationship: source=Node(id='Ml Engineer', type='Person') target=Node(id='Models', type='Concept') type='DESIGNS'\n",
      "Relationship: source=Node(id='Ml Engineer', type='Person') target=Node(id='Models', type='Concept') type='CREATES'\n",
      "Relationship: source=Node(id='Models', type='Concept') target=Node(id='Scalable Solutions', type='Concept') type='CREATES'\n",
      "Relationship: source=Node(id='Models', type='Concept') target=Node(id='Performant Solutions', type='Concept') type='CREATES'\n",
      " \n",
      "graphdoc_idx: 4\n",
      "Len doc_page_content 191\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Note: The exam does not directly assess coding skill. If you have a minimum proficiency in Python and Cloud SQL, you should be able to interpret any questions with code snippets.\n",
      "Register now\n",
      "doc_metadata: {}\n",
      "No. nodes: 0\n",
      "No. relationships: 0\n",
      " \n",
      "graphdoc_idx: 5\n",
      "Len doc_page_content 511\n",
      "No. doc_metadata: 0\n",
      "doc_page_content The Professional Machine Learning Engineer exam does not cover generative AI, as the tools used to develop generative AI-based solutions are evolving quickly. If you are interested in generative AI, please refer to the Introduction to Generative AI Learning Path (all audiences) or the Generative AI for Developers Learning Path (technical audience). If you are a partner, please refer to the Gen AI partner courses: Introduction to Generative AI Learning Path, Generative AI for ML Engineers, and Generative AI\n",
      "doc_metadata: {}\n",
      "No. nodes: 7\n",
      "Node: id='Professional Machine Learning Engineer Exam' type='Exam'\n",
      "Node: id='Generative Ai' type='Ai'\n",
      "Node: id='Generative Ai-Based Solution' type='Solution'\n",
      "Node: id='Generative Ai For Developers Learning Path' type='Learning path'\n",
      "Node: id='Introduction To Generative Ai Learning Path' type='Learning path'\n",
      "Node: id='Gen Ai Partner Courses' type='Course'\n",
      "Node: id='Ml Engineer' type='Profession'\n",
      "No. relationships: 7\n",
      "Relationship: source=Node(id='Professional Machine Learning Engineer Exam', type='Exam') target=Node(id='Generative Ai', type='Ai') type='NOT_COVER'\n",
      "Relationship: source=Node(id='Generative Ai', type='Ai') target=Node(id='Generative Ai-Based Solution', type='Solution') type='NEED_TOOLS_FOR'\n",
      "Relationship: source=Node(id='Generative Ai', type='Ai') target=Node(id='Generative Ai For Developers Learning Path', type='Learning path') type='LEARNING_PATH_FOR'\n",
      "Relationship: source=Node(id='Generative Ai', type='Ai') target=Node(id='Introduction To Generative Ai Learning Path', type='Learning path') type='LEARNING_PATH_FOR'\n",
      "Relationship: source=Node(id='Introduction To Generative Ai Learning Path', type='Learning path') target=Node(id='Gen Ai Partner Courses', type='Course') type='PART_OF'\n",
      "Relationship: source=Node(id='Generative Ai', type='Ai') target=Node(id='Gen Ai Partner Courses', type='Course') type='PART_OF'\n",
      "Relationship: source=Node(id='Gen Ai Partner Courses', type='Course') target=Node(id='Ml Engineer', type='Profession') type='FOR'\n",
      " \n",
      "graphdoc_idx: 6\n",
      "Len doc_page_content 33\n",
      "No. doc_metadata: 0\n",
      "doc_page_content and Generative AI for Developers.\n",
      "doc_metadata: {}\n",
      "No. nodes: 2\n",
      "Node: id='Generative Ai' type='Technology'\n",
      "Node: id='Developers' type='People'\n",
      "No. relationships: 1\n",
      "Relationship: source=Node(id='Generative Ai', type='Technology') target=Node(id='Developers', type='People') type='USED_BY'\n",
      " \n",
      "graphdoc_idx: 7\n",
      "Len doc_page_content 491\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Section 1: Architecting low-code ML solutions (~12% of the exam)\n",
      "1.1 Developing ML models by using BigQuery ML. Considerations include:\n",
      "Building the appropriate BigQuery ML model (e.g., linear and binary classification, regression, time-series, matrix factorization, boosted trees, autoencoders) based on the business problem\n",
      "Feature engineering or selection by using BigQuery ML\n",
      "Generating predictions by using BigQuery ML\n",
      "1.2 Building AI solutions by using ML APIs. Considerations include:\n",
      "doc_metadata: {}\n",
      "No. nodes: 6\n",
      "Node: id='Architecting Low-Code Ml Solutions' type='Section'\n",
      "Node: id='Developing Ml Models By Using Bigquery Ml' type='Subsection'\n",
      "Node: id='Building The Appropriate Bigquery Ml Model (E.G., Linear And Binary Classification, Regression, Time-Series, Matrix Factorization, Boosted Trees, Autoencoders) Based On The Business Problem' type='Paragraph'\n",
      "Node: id='Feature Engineering Or Selection By Using Bigquery Ml' type='Paragraph'\n",
      "Node: id='Generating Predictions By Using Bigquery Ml' type='Paragraph'\n",
      "Node: id='Building Ai Solutions By Using Ml Apis' type='Subsection'\n",
      "No. relationships: 5\n",
      "Relationship: source=Node(id='Architecting Low-Code Ml Solutions', type='Section') target=Node(id='Developing Ml Models By Using Bigquery Ml', type='Subsection') type='CONTAINS'\n",
      "Relationship: source=Node(id='Developing Ml Models By Using Bigquery Ml', type='Subsection') target=Node(id='Building The Appropriate Bigquery Ml Model (E.G., Linear And Binary Classification, Regression, Time-Series, Matrix Factorization, Boosted Trees, Autoencoders) Based On The Business Problem', type='Paragraph') type='CONTAINS'\n",
      "Relationship: source=Node(id='Developing Ml Models By Using Bigquery Ml', type='Subsection') target=Node(id='Feature Engineering Or Selection By Using Bigquery Ml', type='Paragraph') type='CONTAINS'\n",
      "Relationship: source=Node(id='Developing Ml Models By Using Bigquery Ml', type='Subsection') target=Node(id='Generating Predictions By Using Bigquery Ml', type='Paragraph') type='CONTAINS'\n",
      "Relationship: source=Node(id='Architecting Low-Code Ml Solutions', type='Section') target=Node(id='Building Ai Solutions By Using Ml Apis', type='Subsection') type='CONTAINS'\n",
      " \n",
      "graphdoc_idx: 8\n",
      "Len doc_page_content 483\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Building applications by using ML APIs (e.g., Cloud Vision API, Natural Language API, Cloud Speech API, Translation)\n",
      "Building applications by using industry-specific APIs (e.g., Document AI API, Retail API)\n",
      "1.3 Training models by using AutoML. Considerations include:\n",
      "Preparing data for AutoML (e.g., feature selection, data labeling, Tabular Workflows on AutoML)\n",
      "Using available data (e.g., tabular, text, speech, images, videos) to train custom models\n",
      "Using AutoML for tabular data\n",
      "doc_metadata: {}\n",
      "No. nodes: 22\n",
      "Node: id='Building Applications' type='Task'\n",
      "Node: id='Ml Apis' type='Tool'\n",
      "Node: id='Cloud Vision Api' type='Ml api'\n",
      "Node: id='Natural Language Api' type='Ml api'\n",
      "Node: id='Cloud Speech Api' type='Ml api'\n",
      "Node: id='Translation Api' type='Ml api'\n",
      "Node: id='Industry-Specific Apis' type='Tool'\n",
      "Node: id='Document Ai Api' type='Industry-specific api'\n",
      "Node: id='Retail Api' type='Industry-specific api'\n",
      "Node: id='Automl' type='Tool'\n",
      "Node: id='Preparing Data For Automl' type='Task'\n",
      "Node: id='Feature Selection' type='Task'\n",
      "Node: id='Data Labeling' type='Task'\n",
      "Node: id='Tabular Workflows On Automl' type='Tool'\n",
      "Node: id='Using Available Data' type='Task'\n",
      "Node: id='Training Models' type='Task'\n",
      "Node: id='Tabular Data' type='Datatype'\n",
      "Node: id='Text Data' type='Datatype'\n",
      "Node: id='Speech Data' type='Datatype'\n",
      "Node: id='Image Data' type='Datatype'\n",
      "Node: id='Video Data' type='Datatype'\n",
      "Node: id='Custom Models' type='Model'\n",
      "No. relationships: 20\n",
      "Relationship: source=Node(id='Building Applications', type='Task') target=Node(id='Ml Apis', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Building Applications', type='Task') target=Node(id='Industry-Specific Apis', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Ml Apis', type='Tool') target=Node(id='Cloud Vision Api', type='Ml api') type='INCLUDES'\n",
      "Relationship: source=Node(id='Ml Apis', type='Tool') target=Node(id='Natural Language Api', type='Ml api') type='INCLUDES'\n",
      "Relationship: source=Node(id='Ml Apis', type='Tool') target=Node(id='Cloud Speech Api', type='Ml api') type='INCLUDES'\n",
      "Relationship: source=Node(id='Ml Apis', type='Tool') target=Node(id='Translation Api', type='Ml api') type='INCLUDES'\n",
      "Relationship: source=Node(id='Industry-Specific Apis', type='Tool') target=Node(id='Document Ai Api', type='Industry-specific api') type='INCLUDES'\n",
      "Relationship: source=Node(id='Industry-Specific Apis', type='Tool') target=Node(id='Retail Api', type='Industry-specific api') type='INCLUDES'\n",
      "Relationship: source=Node(id='Automl', type='Tool') target=Node(id='Training Models', type='Task') type='USED_FOR'\n",
      "Relationship: source=Node(id='Preparing Data For Automl', type='Task') target=Node(id='Automl', type='Tool') type='USED_FOR'\n",
      "Relationship: source=Node(id='Feature Selection', type='Task') target=Node(id='Preparing Data For Automl', type='Task') type='INCLUDES'\n",
      "Relationship: source=Node(id='Data Labeling', type='Task') target=Node(id='Preparing Data For Automl', type='Task') type='INCLUDES'\n",
      "Relationship: source=Node(id='Tabular Workflows On Automl', type='Tool') target=Node(id='Preparing Data For Automl', type='Task') type='USED_FOR'\n",
      "Relationship: source=Node(id='Using Available Data', type='Task') target=Node(id='Automl', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Tabular Data', type='Datatype') target=Node(id='Using Available Data', type='Task') type='TYPE_OF_DATA'\n",
      "Relationship: source=Node(id='Text Data', type='Datatype') target=Node(id='Using Available Data', type='Task') type='TYPE_OF_DATA'\n",
      "Relationship: source=Node(id='Speech Data', type='Datatype') target=Node(id='Using Available Data', type='Task') type='TYPE_OF_DATA'\n",
      "Relationship: source=Node(id='Image Data', type='Datatype') target=Node(id='Using Available Data', type='Task') type='TYPE_OF_DATA'\n",
      "Relationship: source=Node(id='Video Data', type='Datatype') target=Node(id='Using Available Data', type='Task') type='TYPE_OF_DATA'\n",
      "Relationship: source=Node(id='Automl', type='Tool') target=Node(id='Custom Models', type='Model') type='CREATES'\n",
      " \n",
      "graphdoc_idx: 9\n",
      "Len doc_page_content 471\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Creating forecasting models using AutoML\n",
      "Configuring and debugging trained models\n",
      "Section 2: Collaborating within and across teams to manage data and models (~16% of the exam)\n",
      "2.1 Exploring and preprocessing organization-wide data (e.g., Cloud Storage, BigQuery, Spanner, Cloud SQL, Apache Spark, Apache Hadoop). Considerations include:\n",
      "Organizing different types of data (e.g., tabular, text, speech, images, videos) for efficient training\n",
      "Managing datasets in Vertex AI\n",
      "doc_metadata: {}\n",
      "No. nodes: 0\n",
      "No. relationships: 3\n",
      "Relationship: source=Node(id='Automl', type='Concept') target=Node(id='Forecasting Models', type='Concept') type='CREATING'\n",
      "Relationship: source=Node(id='Forecasting Models', type='Concept') target=Node(id='Trained Models', type='Concept') type='CONFIGURING'\n",
      "Relationship: source=Node(id='Trained Models', type='Concept') target=Node(id='Debugging', type='Concept') type='DEBUGGING'\n",
      " \n",
      "graphdoc_idx: 10\n",
      "Len doc_page_content 488\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Data preprocessing (e.g., Dataflow, TensorFlow Extended [TFX], BigQuery)\n",
      "Creating and consolidating features in Vertex AI Feature Store\n",
      "Privacy implications of data usage and/or collection (e.g., handling sensitive data such as personally identifiable information [PII] and protected health information [PHI])\n",
      "2.2 Model prototyping using Jupyter notebooks. Considerations include:\n",
      "Choosing the appropriate Jupyter backend on Google Cloud (e.g., Vertex AI Workbench, notebooks on Dataproc)\n",
      "doc_metadata: {}\n",
      "No. nodes: 11\n",
      "Node: id='Data Preprocessing' type='Task'\n",
      "Node: id='Dataflow' type='Tool'\n",
      "Node: id='Tensorflow Extended [Tfx]' type='Tool'\n",
      "Node: id='Bigquery' type='Tool'\n",
      "Node: id='Vertex Ai Feature Store' type='Service'\n",
      "Node: id='Privacy Implications Of Data Usage And/Or Collection' type='Task'\n",
      "Node: id='Handling Sensitive Data Such As Personally Identifiable Information [Pii] And Protected Health Information [Phi]' type='Task'\n",
      "Node: id='Model Prototyping Using Jupyter Notebooks' type='Task'\n",
      "Node: id='Jupyter Notebooks' type='Tool'\n",
      "Node: id='Vertex Ai Workbench' type='Tool'\n",
      "Node: id='Notebooks On Dataproc' type='Tool'\n",
      "No. relationships: 9\n",
      "Relationship: source=Node(id='Data Preprocessing', type='Task') target=Node(id='Dataflow', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Data Preprocessing', type='Task') target=Node(id='Tensorflow Extended [Tfx]', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Data Preprocessing', type='Task') target=Node(id='Bigquery', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Vertex Ai Feature Store', type='Service') target=Node(id='Data Preprocessing', type='Task') type='SUPPORTS'\n",
      "Relationship: source=Node(id='Privacy Implications Of Data Usage And/Or Collection', type='Task') target=Node(id='Data Preprocessing', type='Task') type='DEPENDS_ON'\n",
      "Relationship: source=Node(id='Privacy Implications Of Data Usage And/Or Collection', type='Task') target=Node(id='Handling Sensitive Data Such As Personally Identifiable Information [Pii] And Protected Health Information [Phi]', type='Task') type='INCLUDES'\n",
      "Relationship: source=Node(id='Model Prototyping Using Jupyter Notebooks', type='Task') target=Node(id='Jupyter Notebooks', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Model Prototyping Using Jupyter Notebooks', type='Task') target=Node(id='Vertex Ai Workbench', type='Tool') type='USES'\n",
      "Relationship: source=Node(id='Model Prototyping Using Jupyter Notebooks', type='Task') target=Node(id='Notebooks On Dataproc', type='Tool') type='USES'\n",
      " \n",
      "graphdoc_idx: 11\n",
      "Len doc_page_content 506\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Applying security best practices in Vertex AI Workbench\n",
      "Using Spark kernels\n",
      "Integration with code source repositories\n",
      "Developing models in Vertex AI Workbench by using common frameworks (e.g., TensorFlow, PyTorch, sklearn, Spark, JAX)\n",
      "2.3 Tracking and running ML experiments. Considerations include:\n",
      "Choosing the appropriate Google Cloud environment for development and experimentation (e.g., Vertex AI Experiments, Kubeflow Pipelines, Vertex AI TensorBoard with TensorFlow and PyTorch) given the framework\n",
      "doc_metadata: {}\n",
      "No. nodes: 15\n",
      "Node: id='Securitybestpractices' type='Concept'\n",
      "Node: id='Vertexaiworkbench' type='Technology'\n",
      "Node: id='Sparkkernels' type='Technology'\n",
      "Node: id='Codesourcerepositories' type='Concept'\n",
      "Node: id='Mlframeworks' type='Concept'\n",
      "Node: id='Vertexaiexperiments' type='Technology'\n",
      "Node: id='Kubeflowpipelines' type='Technology'\n",
      "Node: id='Vertexaitensorboard' type='Technology'\n",
      "Node: id='Tensorflow' type='Technology'\n",
      "Node: id='Pytorch' type='Technology'\n",
      "Node: id='Sklearn' type='Technology'\n",
      "Node: id='Spark' type='Technology'\n",
      "Node: id='Jaxx' type='Technology'\n",
      "Node: id='Googlecloudenvironment' type='Concept'\n",
      "Node: id='Mlexperiments' type='Concept'\n",
      "No. relationships: 15\n",
      "Relationship: source=Node(id='Securitybestpractices', type='Concept') target=Node(id='Vertexaiworkbench', type='Technology') type='APPLICATION'\n",
      "Relationship: source=Node(id='Vertexaiworkbench', type='Technology') target=Node(id='Sparkkernels', type='Technology') type='USES'\n",
      "Relationship: source=Node(id='Vertexaiworkbench', type='Technology') target=Node(id='Codesourcerepositories', type='Concept') type='INTEGRATION'\n",
      "Relationship: source=Node(id='Vertexaiworkbench', type='Technology') target=Node(id='Mlframeworks', type='Concept') type='DEVELOPMENT'\n",
      "Relationship: source=Node(id='Mlframeworks', type='Concept') target=Node(id='Tensorflow', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Mlframeworks', type='Concept') target=Node(id='Pytorch', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Mlframeworks', type='Concept') target=Node(id='Sklearn', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Mlframeworks', type='Concept') target=Node(id='Spark', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Mlframeworks', type='Concept') target=Node(id='Jaxx', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Mlexperiments', type='Concept') target=Node(id='Googlecloudenvironment', type='Concept') type='CHOICE'\n",
      "Relationship: source=Node(id='Googlecloudenvironment', type='Concept') target=Node(id='Vertexaiexperiments', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Googlecloudenvironment', type='Concept') target=Node(id='Kubeflowpipelines', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Googlecloudenvironment', type='Concept') target=Node(id='Vertexaitensorboard', type='Technology') type='EXAMPLE'\n",
      "Relationship: source=Node(id='Vertexaitensorboard', type='Technology') target=Node(id='Tensorflow', type='Technology') type='WORKS_WITH'\n",
      "Relationship: source=Node(id='Vertexaitensorboard', type='Technology') target=Node(id='Pytorch', type='Technology') type='WORKS_WITH'\n",
      " \n",
      "graphdoc_idx: 12\n",
      "Len doc_page_content 464\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Section 3: Scaling prototypes into ML models (~18% of the exam)\n",
      "3.1 Building models. Considerations include:\n",
      "Choosing ML framework and model architecture\n",
      "Modeling techniques given interpretability requirements\n",
      "3.2 Training models. Considerations include:\n",
      "Organizing training data (e.g., tabular, text, speech, images, videos) on Google Cloud (e.g., Cloud Storage, BigQuery)\n",
      "Ingestion of various file types (e.g., CSV, JSON, images, Hadoop, databases) into training\n",
      "doc_metadata: {}\n",
      "No. nodes: 19\n",
      "Node: id='Scaling Prototypes' type='Topic'\n",
      "Node: id='Building Models' type='Topic'\n",
      "Node: id='Training Models' type='Topic'\n",
      "Node: id='Ml Models' type='Concept'\n",
      "Node: id='Ml Framework' type='Concept'\n",
      "Node: id='Model Architecture' type='Concept'\n",
      "Node: id='Modeling Techniques' type='Concept'\n",
      "Node: id='Interpretability Requirements' type='Concept'\n",
      "Node: id='Training Data' type='Concept'\n",
      "Node: id='Google Cloud' type='Concept'\n",
      "Node: id='Cloud Storage' type='Concept'\n",
      "Node: id='Bigquery' type='Concept'\n",
      "Node: id='File Types' type='Concept'\n",
      "Node: id='Csv' type='Type'\n",
      "Node: id='Json' type='Type'\n",
      "Node: id='Images' type='Type'\n",
      "Node: id='Hadoop' type='Concept'\n",
      "Node: id='Databases' type='Concept'\n",
      "Node: id='Ingestion' type='Process'\n",
      "No. relationships: 18\n",
      "Relationship: source=Node(id='Scaling Prototypes', type='Topic') target=Node(id='Ml Models', type='Concept') type='TRANSFORMATION'\n",
      "Relationship: source=Node(id='Building Models', type='Topic') target=Node(id='Scaling Prototypes', type='Topic') type='SUBTOPIC'\n",
      "Relationship: source=Node(id='Building Models', type='Topic') target=Node(id='Ml Framework', type='Concept') type='REQUIREMENT'\n",
      "Relationship: source=Node(id='Building Models', type='Topic') target=Node(id='Model Architecture', type='Concept') type='REQUIREMENT'\n",
      "Relationship: source=Node(id='Building Models', type='Topic') target=Node(id='Modeling Techniques', type='Concept') type='REQUIREMENT'\n",
      "Relationship: source=Node(id='Modeling Techniques', type='Concept') target=Node(id='Interpretability Requirements', type='Concept') type='CONSTRAINT'\n",
      "Relationship: source=Node(id='Training Models', type='Topic') target=Node(id='Scaling Prototypes', type='Topic') type='SUBTOPIC'\n",
      "Relationship: source=Node(id='Training Models', type='Topic') target=Node(id='Training Data', type='Concept') type='INPUT'\n",
      "Relationship: source=Node(id='Training Data', type='Concept') target=Node(id='Google Cloud', type='Concept') type='LOCATION'\n",
      "Relationship: source=Node(id='Google Cloud', type='Concept') target=Node(id='Cloud Storage', type='Concept') type='COMPONENT'\n",
      "Relationship: source=Node(id='Google Cloud', type='Concept') target=Node(id='Bigquery', type='Concept') type='COMPONENT'\n",
      "Relationship: source=Node(id='Training Data', type='Concept') target=Node(id='File Types', type='Concept') type='COMPOSITION'\n",
      "Relationship: source=Node(id='File Types', type='Concept') target=Node(id='Csv', type='Type') type='ELEMENT'\n",
      "Relationship: source=Node(id='File Types', type='Concept') target=Node(id='Json', type='Type') type='ELEMENT'\n",
      "Relationship: source=Node(id='File Types', type='Concept') target=Node(id='Images', type='Type') type='ELEMENT'\n",
      "Relationship: source=Node(id='Training Data', type='Concept') target=Node(id='Hadoop', type='Concept') type='SOURCE'\n",
      "Relationship: source=Node(id='Training Data', type='Concept') target=Node(id='Databases', type='Concept') type='SOURCE'\n",
      "Relationship: source=Node(id='Training Models', type='Topic') target=Node(id='Ingestion', type='Process') type='PROCESS'\n",
      " \n",
      "graphdoc_idx: 13\n",
      "Len doc_page_content 492\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Training using different SDKs (e.g., Vertex AI custom training, Kubeflow on Google Kubernetes Engine, AutoML, tabular workflows)\n",
      "Using distributed training to organize reliable pipelines\n",
      "Hyperparameter tuning\n",
      "Troubleshooting ML model training failures\n",
      "3.3 Choosing appropriate hardware for training. Considerations include:\n",
      "Evaluation of compute and accelerator options (e.g., CPU, GPU, TPU, edge devices)\n",
      "Distributed training with TPUs and GPUs (e.g., Reduction Server on Vertex AI, Horovod)\n",
      "doc_metadata: {}\n",
      "No. nodes: 11\n",
      "Node: id='Vertex Ai' type='Sdk'\n",
      "Node: id='Kubeflow On Google Kubernetes Engine' type='Sdk'\n",
      "Node: id='Automl' type='Sdk'\n",
      "Node: id='Tabular Workflows' type='Sdk'\n",
      "Node: id='Distributed Training' type='Technique'\n",
      "Node: id='Reliable Pipelines' type='Outcome'\n",
      "Node: id='Hyperparameter Tuning' type='Technique'\n",
      "Node: id='Troubleshooting' type='Process'\n",
      "Node: id='Ml Model Training Failures' type='Problem'\n",
      "Node: id='Tensor Processing Unit' type='Hardware'\n",
      "Node: id='Horovod' type='Framework'\n",
      "No. relationships: 9\n",
      "Relationship: source=Node(id='Vertex Ai', type='Sdk') target=Node(id='Distributed Training', type='Technique') type='USED_FOR'\n",
      "Relationship: source=Node(id='Kubeflow On Google Kubernetes Engine', type='Sdk') target=Node(id='Distributed Training', type='Technique') type='USED_FOR'\n",
      "Relationship: source=Node(id='Automl', type='Sdk') target=Node(id='Distributed Training', type='Technique') type='USED_FOR'\n",
      "Relationship: source=Node(id='Distributed Training', type='Technique') target=Node(id='Reliable Pipelines', type='Outcome') type='PRODUCES'\n",
      "Relationship: source=Node(id='Distributed Training', type='Technique') target=Node(id='Hyperparameter Tuning', type='Technique') type='USED_WITH'\n",
      "Relationship: source=Node(id='Distributed Training', type='Technique') target=Node(id='Troubleshooting', type='Process') type='USED_FOR'\n",
      "Relationship: source=Node(id='Troubleshooting', type='Process') target=Node(id='Ml Model Training Failures', type='Problem') type='ADDRESSES'\n",
      "Relationship: source=Node(id='Tensor Processing Unit', type='Hardware') target=Node(id='Distributed Training', type='Technique') type='USED_FOR'\n",
      "Relationship: source=Node(id='Horovod', type='Framework') target=Node(id='Distributed Training', type='Technique') type='USED_WITH'\n",
      " \n",
      "graphdoc_idx: 14\n",
      "Len doc_page_content 495\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Section 4: Serving and scaling models (~19% of the exam)\n",
      "4.1 Serving models. Considerations include:\n",
      "Batch and online inference (e.g., Vertex AI, Dataflow, BigQuery ML, Dataproc)\n",
      "Using different frameworks (e.g., PyTorch, XGBoost) to serve models\n",
      "Organizing a model registry\n",
      "A/B testing different versions of a model\n",
      "4.2 Scaling online model serving. Considerations include:\n",
      "Vertex AI Feature Store\n",
      "Vertex AI public and private endpoints\n",
      "Choosing appropriate hardware (e.g., CPU, GPU, TPU, edge)\n",
      "doc_metadata: {}\n",
      "No. nodes: 3\n",
      "Node: id='Section4' type='Exam'\n",
      "Node: id='4_1_Servingmodels' type='Serving models'\n",
      "Node: id='4_2_Scalingmodels' type='Scaling online models'\n",
      "No. relationships: 6\n",
      "Relationship: source=Node(id='Section4', type='Exam') target=Node(id='4_1_Servingmodels', type='Serving models') type='HAS'\n",
      "Relationship: source=Node(id='Section4', type='Exam') target=Node(id='4_2_Scalingmodels', type='Scaling online models') type='HAS'\n",
      "Relationship: source=Node(id='4_1_Servingmodels', type='Serving models') target=Node(id='Vertex_Ai', type='Machine_learning_service') type='USES'\n",
      "Relationship: source=Node(id='4_1_Servingmodels', type='Serving models') target=Node(id='Data_Flow', type='Data processing_framework') type='USES'\n",
      "Relationship: source=Node(id='4_1_Servingmodels', type='Serving models') target=Node(id='Bigquery_Ml_Model', type='Machine_learning_service') type='USES'\n",
      "Relationship: source=Node(id='4_1_Servingmodels', type='Serving models') target=Node(id='Data_Proc', type='Cluster management_tool') type='USES'\n",
      " \n",
      "graphdoc_idx: 15\n",
      "Len doc_page_content 507\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Scaling the serving backend based on the throughput (e.g., Vertex AI Prediction, containerized serving)\n",
      "Tuning ML models for training and serving in production (e.g., simplification techniques, optimizing the ML solution for increased performance, latency, memory, throughput)\n",
      "Section 5: Automating and orchestrating ML pipelines (~21% of the exam)\n",
      "5.1 Developing end-to-end ML pipelines. Considerations include:\n",
      "Data and model validation\n",
      "Ensuring consistent data pre-processing between training and serving\n",
      "doc_metadata: {}\n",
      "No. nodes: 3\n",
      "Node: id='Scaling The Serving Backend' type='Process'\n",
      "Node: id='Tuning Ml Models' type='Process'\n",
      "Node: id='Automating And Orchestrating Ml Pipelines' type='Process'\n",
      "No. relationships: 2\n",
      "Relationship: source=Node(id='Scaling The Serving Backend', type='Process') target=Node(id='Tuning Ml Models', type='Process') type='PIPELINE'\n",
      "Relationship: source=Node(id='Tuning Ml Models', type='Process') target=Node(id='Automating And Orchestrating Ml Pipelines', type='Process') type='PIPELINE'\n",
      " \n",
      "graphdoc_idx: 16\n",
      "Len doc_page_content 444\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Hosting third-party pipelines on Google Cloud (e.g., MLFlow)\n",
      "Identifying components, parameters, triggers, and compute needs (e.g., Cloud Build, Cloud Run)\n",
      "Orchestration framework (e.g., Kubeflow Pipelines, Vertex AI Pipelines, Cloud Composer)\n",
      "Hybrid or multicloud strategies\n",
      "System design with TFX components or Kubeflow DSL (e.g., Dataflow)\n",
      "5.2 Automating model retraining. Considerations include:\n",
      "Determining an appropriate retraining policy\n",
      "doc_metadata: {}\n",
      "No. nodes: 11\n",
      "Node: id='Mlflow' type='Thirdpartypipeline'\n",
      "Node: id='Cloudbuild' type='Cloudservice'\n",
      "Node: id='Cloudrun' type='Cloudservice'\n",
      "Node: id='Kubeflowpipelines' type='Orchestrationframework'\n",
      "Node: id='Vertexaipipelines' type='Orchestrationframework'\n",
      "Node: id='Cloudcomposer' type='Orchestrationframework'\n",
      "Node: id='Tfx' type='Systemdesigncomponent'\n",
      "Node: id='Kubeflowdsl' type='Systemdesigncomponent'\n",
      "Node: id='Dataflow' type='Systemdesigncomponent'\n",
      "Node: id='Modelretraining' type='Process'\n",
      "Node: id='Retrainingpolicy' type='Policy'\n",
      "No. relationships: 11\n",
      "Relationship: source=Node(id='Mlflow', type='Thirdpartypipeline') target=Node(id='Cloudbuild', type='Cloudservice') type='HOSTED_ON'\n",
      "Relationship: source=Node(id='Mlflow', type='Thirdpartypipeline') target=Node(id='Cloudrun', type='Cloudservice') type='HOSTED_ON'\n",
      "Relationship: source=Node(id='Cloudbuild', type='Cloudservice') target=Node(id='Kubeflowpipelines', type='Orchestrationframework') type='USED_FOR'\n",
      "Relationship: source=Node(id='Cloudrun', type='Cloudservice') target=Node(id='Kubeflowpipelines', type='Orchestrationframework') type='USED_FOR'\n",
      "Relationship: source=Node(id='Cloudbuild', type='Cloudservice') target=Node(id='Vertexaipipelines', type='Orchestrationframework') type='USED_FOR'\n",
      "Relationship: source=Node(id='Cloudrun', type='Cloudservice') target=Node(id='Vertexaipipelines', type='Orchestrationframework') type='USED_FOR'\n",
      "Relationship: source=Node(id='Cloudbuild', type='Cloudservice') target=Node(id='Cloudcomposer', type='Orchestrationframework') type='USED_FOR'\n",
      "Relationship: source=Node(id='Cloudrun', type='Cloudservice') target=Node(id='Cloudcomposer', type='Orchestrationframework') type='USED_FOR'\n",
      "Relationship: source=Node(id='Tfx', type='Systemdesigncomponent') target=Node(id='Dataflow', type='Systemdesigncomponent') type='USED_WITH'\n",
      "Relationship: source=Node(id='Kubeflowdsl', type='Systemdesigncomponent') target=Node(id='Dataflow', type='Systemdesigncomponent') type='USED_WITH'\n",
      "Relationship: source=Node(id='Modelretraining', type='Process') target=Node(id='Retrainingpolicy', type='Policy') type='DETERMINED_BY'\n",
      " \n",
      "graphdoc_idx: 17\n",
      "Len doc_page_content 445\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Continuous integration and continuous delivery (CI/CD) model deployment (e.g., Cloud Build, Jenkins)\n",
      "5.3 Tracking and auditing metadata. Considerations include: \n",
      "Tracking and comparing model artifacts and versions (e.g., Vertex AI Experiments, Vertex ML Metadata)\n",
      "Hooking into model and dataset versioning\n",
      "Model and data lineage\n",
      "Section 6: Monitoring ML solutions (~14% of the exam)\n",
      "6.1 Identifying risks to ML solutions. Considerations include:\n",
      "doc_metadata: {}\n",
      "No. nodes: 11\n",
      "Node: id='Model Deployment Approaches' type='Model deployment'\n",
      "Node: id='Version Control' type='Version control'\n",
      "Node: id='Vertex Ai' type='Vertex ai'\n",
      "Node: id='Vertex Ml Metadata' type='Vertex ml metadata'\n",
      "Node: id='Cloud Build' type='Cloud build'\n",
      "Node: id='Jenkins' type='Jenkins'\n",
      "Node: id='Monitoring' type='Monitoring'\n",
      "Node: id='Ml Solutions' type='Ml solutions'\n",
      "Node: id='Data Lineage' type='Data lineage'\n",
      "Node: id='Metadata Tracking' type='Metadata tracking'\n",
      "Node: id='Ci/Cd' type='Ci/cd'\n",
      "No. relationships: 11\n",
      "Relationship: source=Node(id='Model Deployment', type='Model deployment') target=Node(id='Continuous Integration & Delivery', type='Ci/cd') type='TYPE'\n",
      "Relationship: source=Node(id='Continuous Integration & Delivery', type='Ci/cd') target=Node(id='Model Deployment', type='Model deployment') type='DEPLOYMENT'\n",
      "Relationship: source=Node(id='Model Deployment Approaches', type='Model deployment') target=Node(id='Cloud Build', type='Cloud build') type='APPROACH'\n",
      "Relationship: source=Node(id='Model Deployment Approaches', type='Model deployment') target=Node(id='Jenkins', type='Jenkins') type='APPROACH'\n",
      "Relationship: source=Node(id='Version Control', type='Version control') target=Node(id='Model Deployment', type='Model deployment') type='ENABLES'\n",
      "Relationship: source=Node(id='Vertex Ai', type='Vertex ai') target=Node(id='Tracking And Comparing Model Artifacts', type='Tracking and comparing model artifacts') type='SUPPORTS'\n",
      "Relationship: source=Node(id='Vertex Ml Metadata', type='Vertex ml metadata') target=Node(id='Tracking And Comparing Model Artifacts', type='Tracking and comparing model artifacts') type='SUPPORTS'\n",
      "Relationship: source=Node(id='Model Deployment', type='Model deployment') target=Node(id='Model And Data Lineage', type='Model and data lineage') type='GENERATES'\n",
      "Relationship: source=Node(id='Model Deployment', type='Model deployment') target=Node(id='Model And Dataset Versioning', type='Model and dataset versioning') type='FACILITATES'\n",
      "Relationship: source=Node(id='Monitoring', type='Monitoring') target=Node(id='Risk Identification', type='Risk identification') type='SUPPORTS'\n",
      "Relationship: source=Node(id='Risk Identification', type='Risk identification') target=Node(id='Ml Solutions', type='Ml solutions') type='IDENTIFIES_RISKS_FOR'\n",
      " \n",
      "graphdoc_idx: 18\n",
      "Len doc_page_content 507\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Building secure ML systems (e.g., protecting against unintentional exploitation of data or models, hacking)\n",
      "Aligning with Googles Responsible AI practices (e.g., biases)\n",
      "Assessing ML solution readiness (e.g., data bias, fairness)\n",
      "Model explainability on Vertex AI (e.g., Vertex AI Prediction)\n",
      "6.2 Monitoring, testing, and troubleshooting ML solutions. Considerations include:\n",
      "Establishing continuous evaluation metrics (e.g., Vertex AI Model Monitoring, Explainable AI)\n",
      "Monitoring for training-serving skew\n",
      "doc_metadata: {}\n",
      "No. nodes: 11\n",
      "Node: id='Machine Learning Systems' type='Topic'\n",
      "Node: id='Responsible Ai' type='Topic'\n",
      "Node: id='Ml Solution Readiness' type='Topic'\n",
      "Node: id='Model Explainability' type='Topic'\n",
      "Node: id='Monitoring, Testing, Troubleshooting' type='Topic'\n",
      "Node: id='Evaluation Metrics' type='Topic'\n",
      "Node: id='Vertex Ai' type='Product'\n",
      "Node: id='Vertex Ai Prediction' type='Product'\n",
      "Node: id='Vertex Ai Model Monitoring' type='Product'\n",
      "Node: id='Explainable Ai' type='Product'\n",
      "Node: id='Training-Serving Skew' type='Topic'\n",
      "No. relationships: 13\n",
      "Relationship: source=Node(id='Building Secure Ml Systems', type='Topic') target=Node(id='Machine Learning Systems', type='Topic') type='IS'\n",
      "Relationship: source=Node(id='Building Secure Ml Systems', type='Topic') target=Node(id='Responsible Ai', type='Topic') type='IS'\n",
      "Relationship: source=Node(id='Aligning With GoogleS Responsible Ai Practices', type='Topic') target=Node(id='Responsible Ai', type='Topic') type='IS'\n",
      "Relationship: source=Node(id='Assessing Ml Solution Readiness', type='Topic') target=Node(id='Ml Solution Readiness', type='Topic') type='IS'\n",
      "Relationship: source=Node(id='Model Explainability On Vertex Ai', type='Topic') target=Node(id='Model Explainability', type='Topic') type='IS'\n",
      "Relationship: source=Node(id='Model Explainability On Vertex Ai', type='Topic') target=Node(id='Vertex Ai', type='Product') type='ON'\n",
      "Relationship: source=Node(id='Model Explainability On Vertex Ai', type='Topic') target=Node(id='Vertex Ai Prediction', type='Product') type='ON'\n",
      "Relationship: source=Node(id='6.2 Monitoring, Testing, And Troubleshooting Ml Solutions', type='Topic') target=Node(id='Monitoring, Testing, Troubleshooting', type='Topic') type='IS'\n",
      "Relationship: source=Node(id='Establishing Continuous Evaluation Metrics', type='Topic') target=Node(id='Evaluation Metrics', type='Topic') type='IS'\n",
      "Relationship: source=Node(id='Establishing Continuous Evaluation Metrics', type='Topic') target=Node(id='Vertex Ai', type='Product') type='ON'\n",
      "Relationship: source=Node(id='Establishing Continuous Evaluation Metrics', type='Topic') target=Node(id='Vertex Ai Model Monitoring', type='Product') type='ON'\n",
      "Relationship: source=Node(id='Establishing Continuous Evaluation Metrics', type='Topic') target=Node(id='Explainable Ai', type='Product') type='ON'\n",
      "Relationship: source=Node(id='Monitoring For Training-Serving Skew', type='Topic') target=Node(id='Training-Serving Skew', type='Topic') type='IS'\n",
      " \n",
      "graphdoc_idx: 19\n",
      "Len doc_page_content 169\n",
      "No. doc_metadata: 0\n",
      "doc_page_content Monitoring for feature attribution drift\n",
      "Monitoring model performance against baselines, simpler models, and across the time dimension\n",
      "Common training and serving errors\n",
      "doc_metadata: {}\n",
      "No. nodes: 7\n",
      "Node: id='Feature Attribution Drift' type='Concept'\n",
      "Node: id='Model Performance' type='Concept'\n",
      "Node: id='Baselines' type='Concept'\n",
      "Node: id='Simpler Models' type='Concept'\n",
      "Node: id='Time Dimension' type='Concept'\n",
      "Node: id='Training Errors' type='Concept'\n",
      "Node: id='Serving Errors' type='Concept'\n",
      "No. relationships: 6\n",
      "Relationship: source=Node(id='Feature Attribution Drift', type='Concept') target=Node(id='Model Performance', type='Concept') type='MONITORED'\n",
      "Relationship: source=Node(id='Model Performance', type='Concept') target=Node(id='Baselines', type='Concept') type='COMPARED_AGAINST'\n",
      "Relationship: source=Node(id='Model Performance', type='Concept') target=Node(id='Simpler Models', type='Concept') type='COMPARED_AGAINST'\n",
      "Relationship: source=Node(id='Model Performance', type='Concept') target=Node(id='Time Dimension', type='Concept') type='MONITORED_ACROSS'\n",
      "Relationship: source=Node(id='Training Errors', type='Concept') target=Node(id='Model Performance', type='Concept') type='AFFECTING'\n",
      "Relationship: source=Node(id='Serving Errors', type='Concept') target=Node(id='Model Performance', type='Concept') type='AFFECTING'\n"
     ]
    }
   ],
   "source": [
    "print_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550372a-5e4c-4719-966f-58bb04d19ccd",
   "metadata": {},
   "source": [
    "**Enter Node4J**\n",
    "\n",
    "Node4J Connectivity\n",
    "\n",
    "Requires singing up for free version.\n",
    "\n",
    "DB Will be stopped if not recently used and will require resuming else will fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2e2c285-73ed-41c7-ac46-eea0d3aec3b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "os.environ[\"NEO4J_URI\"] = \"neo4j+s://a657168d.databases.neo4j.io\"\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\n",
    "os.environ[\"NEO4J_PASSWORD\"] = \"VM3A9Mz6usNT99nLs_lqQssfVK8JxeD81DnEiXlDkZU\"\n",
    "\n",
    "graph = Neo4jGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48151b0d-29cd-4c1b-b572-bab3a3049f9d",
   "metadata": {},
   "source": [
    "**Add to GraphDB**\n",
    "\n",
    "This statement loads Nodes & Relatonships into Node4J\n",
    "\n",
    "Thence they can be viewed/manipulated directly on the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3082a075-0722-49d0-98c8-e9b908b52284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a3d52-26bb-4b2c-aa37-1930fcb38f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
