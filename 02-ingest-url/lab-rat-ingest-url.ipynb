{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be executed in a GCP Notebook, and so checked in/out fo GitHub. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ingest Website to Graph DB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Source Data - GCP AI Exam Guide  \n",
    "gcp_ai_examguide_url = 'https://cloud.google.com/learn/certification/guides/machine-learning-engineer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Text Ingestion from Web site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation: from https://medium.datadriveninvestor.com/unlock-web-data-with-efficiency-master-web-scraping-with-langchain-python-1b19220ecbb4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q langchain-openai langchain playwright beautifulsoup4\n",
    "playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H/L Imports: from https://medium.datadriveninvestor.com/unlock-web-data-with-efficiency-master-web-scraping-with-langchain-python-1b19220ecbb4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "#from langchain.llms import OpenAI\n",
    "from langchain.agents import Text2TextAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L/L Imports: from https://medium.datadriveninvestor.com/unlock-web-data-with-efficiency-master-web-scraping-with-langchain-python-1b19220ecbb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.loader import TextLoader\n",
    "from langchain.splitter import CharacterTextSplitter\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "#from langchain.chroma import Chroma\n",
    "#from langchain.llm import ChatOpenAI\n",
    "#from langchain.compressor import LLMChainExtractor\n",
    "#from langchain.retriever import ContextualCompressionRetriever\n",
    "\n",
    "# Load and split documents\n",
    "loader = TextLoader('path_to_your_file')\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation: from https://dev.to/ranjancse/web-scraping-with-langchain-and-html2text-5edl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain playwright beautifulsoup4 html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H/L IMport: from https://dev.to/ranjancse/web-scraping-with-langchain-and-html2text-5edl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html2text\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Snippet: from https://dev.to/ranjancse/web-scraping-with-langchain-and-html2text-5edl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def do_webscraping(link):\n",
    "    try:\n",
    "        urls = [link]\n",
    "        loader = AsyncHtmlLoader(urls)\n",
    "        docs = loader.load()\n",
    "\n",
    "        html2text_transformer = Html2TextTransformer()\n",
    "        docs_transformed = html2text_transformer.transform_documents(docs)\n",
    "\n",
    "        if docs_transformed != None and len(docs_transformed) > 0:\n",
    "            metadata = docs_transformed[0].metadata\n",
    "            title = metadata.get('title', '')\n",
    "            return {\n",
    "                'summary': docs_transformed[0].page_content,\n",
    "                'title': title,\n",
    "                'metadata': metadata,\n",
    "                'clean_content': html2text.html2text(docs_transformed[0].page_content)\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Data: from https://dev.to/ranjancse/web-scraping-with-langchain-and-html2text-5edl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_results = ['https://www.yelp.com/search?cflt=seafood&find_loc=Mountain+View%2C+CA+94043',\n",
    " 'https://www.yelp.com/search?cflt=seafood&find_loc=Mountain+View%2C+CA',\n",
    " 'https://www.opentable.com/cuisine/best-seafood-restaurants-mountain-view-ca']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution/Demo: from https://dev.to/ranjancse/web-scraping-with-langchain-and-html2text-5edl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in google_search_results:\n",
    "  print(link)\n",
    "  response = await do_webscraping(link)\n",
    "  if response != None:\n",
    "    structured_response.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation: from https://python.langchain.com/docs/integrations/document_transformers/html2text/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  html2text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H/L Imports: from https://python.langchain.com/docs/integrations/document_transformers/html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Snippet 1: from https://python.langchain.com/docs/integrations/document_transformers/html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H/L Imports 2: from https://python.langchain.com/docs/integrations/document_transformers/html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import Html2TextTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Snippet 2: from https://python.langchain.com/docs/integrations/document_transformers/html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Output: from https://python.langchain.com/docs/integrations/document_transformers/html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_transformed[0].page_content[1000:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation: from https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade --user google-cloud-aiplatform==1.36.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H/L Imports: from https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb \n",
    "NB NO LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg\n",
    "import vertexai\n",
    "\n",
    "from google.api_core import retry\n",
    "from vertexai.language_models import TextEmbeddingModel, TextGenerationModel\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init GCP Vertex: from https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Snippet: --> List of URLs: from https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/search/retrieval-augmented-generation/examples/URLs.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # The request was successful, and the content is in response.text\n",
    "    content = response.text\n",
    "\n",
    "URLS = [line.strip() for line in content.splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Snippet: url(s) --> Text: from https://github.com/GoogleCloudPlatform/generative-ai/blob/main/search/retrieval-augmented-generation/examples/rag_google_documentation.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a Google documentation URL, retrieve a list of all text chunks within h2 sections\n",
    "def get_sections(url: str) -> list[str]:\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    sections = []\n",
    "    paragraphs = []\n",
    "\n",
    "    body_div = soup.find(\"div\", class_=\"devsite-article-body\")\n",
    "    for child in body_div.findChildren():\n",
    "        if child.name == \"p\":\n",
    "            paragraphs.append(child.get_text().strip())\n",
    "        if child.name == \"h2\":\n",
    "            sections.append(\" \".join(paragraphs))\n",
    "            break\n",
    "\n",
    "    for header in soup.find_all(\"h2\"):\n",
    "        paragraphs = []\n",
    "        nextNode = header.nextSibling\n",
    "        while nextNode:\n",
    "            if isinstance(nextNode, Tag):\n",
    "                if nextNode.name in {\"p\", \"ul\"}:\n",
    "                    paragraphs.append(nextNode.get_text().strip())\n",
    "                elif nextNode.name == \"h2\":\n",
    "                    sections.append(\" \".join(paragraphs))\n",
    "                    break\n",
    "            nextNode = nextNode.nextSibling\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [t for url in URLS for t in get_sections(url) if t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = [len(t) for t in all_text]\n",
    "pd.DataFrame(text_lengths).hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
